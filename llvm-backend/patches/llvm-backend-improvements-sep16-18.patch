diff --git a/llvm/lib/Target/SLOW32/CMakeLists.txt b/llvm/lib/Target/SLOW32/CMakeLists.txt
index 3009c6ec9..512e671cd 100644
--- a/llvm/lib/Target/SLOW32/CMakeLists.txt
+++ b/llvm/lib/Target/SLOW32/CMakeLists.txt
@@ -16,6 +16,10 @@ add_llvm_target(SLOW32CodeGen
   SLOW32FrameLowering.cpp
   SLOW32ISelDAGToDAG.cpp
   SLOW32ISelLowering.cpp
+  SLOW32ISelLoweringCombines.cpp
+  SLOW32ISelLoweringInlineAsm.cpp
+  SLOW32ISelLoweringVarargs.cpp
+  SLOW32LoadAddrOpt.cpp
   SLOW32InstrInfo.cpp
   SLOW32MachineFunctionInfo.cpp
   SLOW32MCInstLowering.cpp
@@ -40,4 +44,4 @@ add_llvm_target(SLOW32CodeGen
 )
 
 add_subdirectory(MCTargetDesc)
-add_subdirectory(TargetInfo)
\ No newline at end of file
+add_subdirectory(TargetInfo)
diff --git a/llvm/lib/Target/SLOW32/MCTargetDesc/SLOW32InstPrinter.cpp b/llvm/lib/Target/SLOW32/MCTargetDesc/SLOW32InstPrinter.cpp
index 38dc8ccdd..5f33cf664 100644
--- a/llvm/lib/Target/SLOW32/MCTargetDesc/SLOW32InstPrinter.cpp
+++ b/llvm/lib/Target/SLOW32/MCTargetDesc/SLOW32InstPrinter.cpp
@@ -7,6 +7,7 @@
 //===----------------------------------------------------------------------===//
 
 #include "SLOW32InstPrinter.h"
+#include "llvm/ADT/StringRef.h"
 #include "llvm/MC/MCAsmInfo.h"
 #include "llvm/MC/MCExpr.h"
 #include "llvm/MC/MCInst.h"
@@ -15,7 +16,6 @@
 #include "llvm/MC/MCSymbol.h"
 #include "llvm/Support/Casting.h"
 #include "llvm/Support/raw_ostream.h"
-#include <cstring>
 
 using namespace llvm;
 
@@ -42,12 +42,14 @@ void SLOW32InstPrinter::printOperand(const MCInst *MI, unsigned OpNum,
   if (Op.isReg()) {
     printRegName(O, Op.getReg());
   } else if (Op.isImm()) {
-    // Check if this is an ADDI instruction that needs sign-extended 12-bit immediate
-    const char *Mnemonic = MII.getName(MI->getOpcode()).data();
+    // Check if this is an ADDI instruction that needs sign-extended 12-bit
+    // immediate. MCInstrInfo returns StringRef, so use it directly rather than
+    // assuming a null-terminated C string.
+    StringRef Mnemonic = MII.getName(MI->getOpcode());
 
     // Special handling for ADDI instruction's immediate operand
     // ADDI uses 12-bit signed immediates that need sign extension when printing
-    if (Mnemonic && strcmp(Mnemonic, "ADDI") == 0 && OpNum == 2) {
+    if (Mnemonic == "ADDI" && OpNum == 2) {
       int64_t Imm = Op.getImm();
       // Mask to 12 bits first in case larger value is stored
       Imm = Imm & 0xFFF;
@@ -60,40 +62,30 @@ void SLOW32InstPrinter::printOperand(const MCInst *MI, unsigned OpNum,
       O << Op.getImm();
     }
   } else if (Op.isExpr()) {
-    // For SLOW32, we need to handle %hi and %lo for global addresses
-    // We check the instruction mnemonic to determine if this needs %hi or %lo
-    // This is a simplified approach - ideally we'd use target flags from MCInstLowering
-    
-    // Get the instruction name from the generated instruction info
-    const MCInstrDesc &Desc = MII.get(MI->getOpcode());
-    const char *Mnemonic = MII.getName(MI->getOpcode()).data();
-    
+    // For SLOW32, we need to handle %hi/%lo fixups when the operand is a
+    // symbolic expression. We drive this off the opcode until we plumb
+    // dedicated MCExprs through the MC layer.
+    StringRef Mnemonic = MII.getName(MI->getOpcode());
+
     // Check if this is a LUI instruction (needs %hi)
-    if (Mnemonic && strstr(Mnemonic, "LUI")) {
+    if (Mnemonic == "LUI") {
       O << "%hi(";
       MAI.printExpr(O, *Op.getExpr());
       O << ")";
     }
-    // Check if this is an ORI, ADDI or LI instruction following a pattern (needs %lo)
-    else if (Mnemonic && (strstr(Mnemonic, "ORI") || strstr(Mnemonic, "ADDI") || strstr(Mnemonic, "LI"))) {
-      // Check if this is for the lower part of an address
-      // by looking at the register operand (should be r0 for %lo)
-      if (OpNum > 1 && MI->getOperand(1).isReg()) {
-        MCRegister Reg = MI->getOperand(1).getReg();
-        const char *RegName = getRegisterName(Reg);
-        if (RegName && strcmp(RegName, "r0") == 0) {
-          O << "%lo(";
-          MAI.printExpr(O, *Op.getExpr());
-          O << ")";
-        } else {
-          // Regular expression without %lo
-          MAI.printExpr(O, *Op.getExpr());
-        }
-      } else {
-        // Regular expression
-        MAI.printExpr(O, *Op.getExpr());
-      }
-    } else {
+    // ADDI materialises the low 12 bits of an address.
+    else if (Mnemonic == "ADDI") {
+      O << "%lo(";
+      MAI.printExpr(O, *Op.getExpr());
+      O << ")";
+    }
+    // Keep supporting legacy ORI-based materialisation for now.
+    else if (Mnemonic == "ORI") {
+      O << "%lo(";
+      MAI.printExpr(O, *Op.getExpr());
+      O << ")";
+    }
+    else {
       // For other instructions, print the expression normally
       MAI.printExpr(O, *Op.getExpr());
     }
@@ -124,4 +116,4 @@ void SLOW32InstPrinter::printBranchTarget(const MCInst *MI, unsigned OpNum,
     // Fallback to regular operand printing
     printOperand(MI, OpNum, STI, O);
   }
-}
\ No newline at end of file
+}
diff --git a/llvm/lib/Target/SLOW32/SLOW32.h b/llvm/lib/Target/SLOW32/SLOW32.h
index e934f38c3..0f4f6000c 100644
--- a/llvm/lib/Target/SLOW32/SLOW32.h
+++ b/llvm/lib/Target/SLOW32/SLOW32.h
@@ -14,6 +14,7 @@
 #ifndef LLVM_LIB_TARGET_SLOW32_SLOW32_H
 #define LLVM_LIB_TARGET_SLOW32_SLOW32_H
 
+#include "llvm/PassRegistry.h"
 #include "llvm/Target/TargetMachine.h"
 
 namespace llvm {
@@ -21,6 +22,8 @@ class FunctionPass;
 class SLOW32TargetMachine;
 
 FunctionPass *createSLOW32ISelDag(SLOW32TargetMachine &TM);
+FunctionPass *createSLOW32LoadAddrOptPass();
+void initializeSLOW32LoadAddrOptPass(PassRegistry &);
 // FunctionPass *createSLOW32NormalizeCallTailsPass();  // Disabled - not needed after upstream fixes
 // FunctionPass *createSLOW32RepairCFGFromTerminatorsPass();  // Disabled - not needed after upstream fixes
 
@@ -45,4 +48,4 @@ enum TargetFlags {
 #define GET_SUBTARGETINFO_ENUM
 #include "SLOW32GenSubtargetInfo.inc"
 
-#endif
\ No newline at end of file
+#endif
diff --git a/llvm/lib/Target/SLOW32/SLOW32FrameLowering.cpp b/llvm/lib/Target/SLOW32/SLOW32FrameLowering.cpp
index 1d80d80c2..8fa294f90 100644
--- a/llvm/lib/Target/SLOW32/SLOW32FrameLowering.cpp
+++ b/llvm/lib/Target/SLOW32/SLOW32FrameLowering.cpp
@@ -2,14 +2,55 @@
 #include "SLOW32FrameLowering.h"
 #include "SLOW32.h"
 #include "SLOW32InstrInfo.h"
+#include "SLOW32RegisterInfo.h"
 #include "SLOW32Subtarget.h"
 #include "llvm/CodeGen/MachineFrameInfo.h"
 #include "llvm/CodeGen/MachineFunction.h"
 #include "llvm/CodeGen/MachineInstrBuilder.h"
 #include "llvm/CodeGen/MachineRegisterInfo.h"
+#include <algorithm>
+#include <cassert>
 
 using namespace llvm;
 
+namespace {
+
+static const Register StackPtr = SLOW32::R29;
+static const Register FramePtr = SLOW32::R30;
+static const Register LinkReg  = SLOW32::R31;
+
+static void emitAddImmediateChain(MachineBasicBlock &MBB,
+                                  MachineBasicBlock::iterator InsertPt,
+                                  const DebugLoc &DL,
+                                  const SLOW32InstrInfo &TII,
+                                  Register DestReg, Register SrcReg,
+                                  int64_t Amount) {
+  if (DestReg != SrcReg) {
+    BuildMI(MBB, InsertPt, DL, TII.get(SLOW32::ADD), DestReg)
+        .addReg(SrcReg)
+        .addReg(SLOW32::R0);
+  }
+
+  Register CurrReg = DestReg;
+  int64_t Remaining = Amount;
+
+  while (Remaining != 0) {
+    int64_t Step;
+    if (Remaining > 0)
+      Step = std::min<int64_t>(Remaining, 2047);
+    else
+      Step = std::max<int64_t>(Remaining, -2048);
+
+    BuildMI(MBB, InsertPt, DL, TII.get(SLOW32::ADDI), CurrReg)
+        .addReg(CurrReg)
+        .addImm(Step);
+
+    Remaining -= Step;
+  }
+}
+
+} // end anonymous namespace
+
 void SLOW32FrameLowering::emitPrologue(MachineFunction &MF, MachineBasicBlock &MBB) const {
   MachineFrameInfo &MFI = MF.getFrameInfo();
   const SLOW32InstrInfo *TII =
@@ -18,15 +59,19 @@ void SLOW32FrameLowering::emitPrologue(MachineFunction &MF, MachineBasicBlock &M
   MachineBasicBlock::iterator MBBI = MBB.begin();
   DebugLoc DL;
 
-  // TEMPORARILY DISABLED: Marking physical registers as live-in
-  // Causes assertion in Machine Copy Propagation pass
-  // if (&MBB == &MF.front()) {
-  //   for (Register R : {SLOW32::SP, SLOW32::FP, SLOW32::LR}) {
-  //     if (!MBB.isLiveIn(R)) {
-  //       MBB.addLiveIn(R);
-  //     }
-  //   }
-  // }
+  assert(SLOW32::GPRRegClass.contains(StackPtr) && "SP not in GPR");
+  assert(SLOW32::GPRRegClass.contains(FramePtr) && "FP not in GPR");
+  assert(SLOW32::GPRRegClass.contains(LinkReg) && "LR not in GPR");
+
+  if (&MBB == &MF.front()) {
+    MachineRegisterInfo &MRI = MF.getRegInfo();
+    for (Register R : {StackPtr, FramePtr, LinkReg}) {
+      if (!MRI.isLiveIn(R))
+        MRI.addLiveIn(R);
+      if (!MBB.isLiveIn(R))
+        MBB.addLiveIn(R);
+    }
+  }
   
   // Calculate frame size
   uint64_t FrameSize = MFI.getStackSize();
@@ -37,27 +82,25 @@ void SLOW32FrameLowering::emitPrologue(MachineFunction &MF, MachineBasicBlock &M
   // Adjust for saving FP and LR (2 * 4 bytes)
   FrameSize = FrameSize + 8;
   
-  // Adjust stack pointer: sp = sp - framesize
-  BuildMI(MBB, MBBI, DL, TII->get(SLOW32::ADDI), SLOW32::SP)
-      .addReg(SLOW32::SP)
-      .addImm(-FrameSize);
-  
+  // Adjust stack pointer: sp = sp - framesize (may require multiple steps)
+  emitAddImmediateChain(MBB, MBBI, DL, *TII, StackPtr, StackPtr,
+                        -static_cast<int64_t>(FrameSize));
+
   // Save old frame pointer: stw sp+4, fp
   BuildMI(MBB, MBBI, DL, TII->get(SLOW32::STW))
-      .addReg(SLOW32::SP)
-      .addReg(SLOW32::FP)
+      .addReg(StackPtr)
+      .addReg(FramePtr)
       .addImm(4);
-  
+
   // Save link register: stw sp+0, lr
   BuildMI(MBB, MBBI, DL, TII->get(SLOW32::STW))
-      .addReg(SLOW32::SP)
-      .addReg(SLOW32::LR)
+      .addReg(StackPtr)
+      .addReg(LinkReg)
       .addImm(0);
-  
+
   // Setup new frame pointer: fp = sp + framesize (point to old fp location)
-  BuildMI(MBB, MBBI, DL, TII->get(SLOW32::ADDI), SLOW32::FP)
-      .addReg(SLOW32::SP)
-      .addImm(FrameSize);
+  emitAddImmediateChain(MBB, MBBI, DL, *TII, FramePtr, StackPtr,
+                        static_cast<int64_t>(FrameSize));
 }
 
 void SLOW32FrameLowering::emitEpilogue(MachineFunction &MF, MachineBasicBlock &MBB) const {
@@ -78,19 +121,18 @@ void SLOW32FrameLowering::emitEpilogue(MachineFunction &MF, MachineBasicBlock &M
   
   // SP currently points to bottom of frame
   // Restore link register from SP+0
-  BuildMI(MBB, MBBI, DL, TII->get(SLOW32::LDW), SLOW32::LR)
-      .addReg(SLOW32::SP)
+  BuildMI(MBB, MBBI, DL, TII->get(SLOW32::LDW), LinkReg)
+      .addReg(StackPtr)
       .addImm(0);
-  
+
   // Restore old frame pointer from SP+4
-  BuildMI(MBB, MBBI, DL, TII->get(SLOW32::LDW), SLOW32::FP)
-      .addReg(SLOW32::SP)
+  BuildMI(MBB, MBBI, DL, TII->get(SLOW32::LDW), FramePtr)
+      .addReg(StackPtr)
       .addImm(4);
-  
+
   // Restore stack pointer: sp = sp + framesize
-  BuildMI(MBB, MBBI, DL, TII->get(SLOW32::ADDI), SLOW32::SP)
-      .addReg(SLOW32::SP)
-      .addImm(FrameSize);
+  emitAddImmediateChain(MBB, MBBI, DL, *TII, StackPtr, StackPtr,
+                        static_cast<int64_t>(FrameSize));
 }
 
 MachineBasicBlock::iterator
@@ -108,14 +150,12 @@ SLOW32FrameLowering::eliminateCallFramePseudoInstr(
       // Adjust stack pointer
       if (I->getOpcode() == SLOW32::ADJCALLSTACKDOWN) {
         // Allocate space for outgoing arguments
-        BuildMI(MBB, I, I->getDebugLoc(), TII->get(SLOW32::ADDI), SLOW32::SP)
-            .addReg(SLOW32::SP)
-            .addImm(-Amount);
+        emitAddImmediateChain(MBB, I, I->getDebugLoc(), *TII, StackPtr,
+                              StackPtr, -Amount);
       } else {
         // Deallocate space for outgoing arguments
-        BuildMI(MBB, I, I->getDebugLoc(), TII->get(SLOW32::ADDI), SLOW32::SP)
-            .addReg(SLOW32::SP)
-            .addImm(Amount);
+        emitAddImmediateChain(MBB, I, I->getDebugLoc(), *TII, StackPtr,
+                              StackPtr, Amount);
       }
     }
   }
diff --git a/llvm/lib/Target/SLOW32/SLOW32ISelDAGToDAG.cpp b/llvm/lib/Target/SLOW32/SLOW32ISelDAGToDAG.cpp
index bb336b62d..383107322 100644
--- a/llvm/lib/Target/SLOW32/SLOW32ISelDAGToDAG.cpp
+++ b/llvm/lib/Target/SLOW32/SLOW32ISelDAGToDAG.cpp
@@ -13,6 +13,7 @@
 #include "llvm/CodeGen/SelectionDAGISel.h"
 #include "llvm/CodeGen/SelectionDAGNodes.h"
 #include "llvm/Support/Debug.h"
+#include "llvm/Support/MathExtras.h"
 
 using namespace llvm;
 
@@ -24,91 +25,99 @@ public:
   SLOW32DAGToDAGISel(SLOW32TargetMachine &TM)
       : SelectionDAGISel(TM) {}
 
+private:
+  SDValue createLoadAddrForGlobal(const SDLoc &DL,
+                                  GlobalAddressSDNode *GA,
+                                  int64_t AdditionalOffset) const {
+    int64_t TotalOffset = GA->getOffset() + AdditionalOffset;
+    SDValue BaseSym = CurDAG->getTargetGlobalAddress(
+        GA->getGlobal(), DL, MVT::i32, TotalOffset, GA->getTargetFlags());
+    SDNode *LoadAddr =
+        CurDAG->getMachineNode(SLOW32::LOAD_ADDR, DL, MVT::i32, BaseSym);
+    return SDValue(LoadAddr, 0);
+  }
+
+  SDValue tryFoldLoadAddrAddend(SDValue LoadAddr, int64_t ExtraOffset,
+                                const SDLoc &DL) const {
+    SDNode *LoadAddrNode = LoadAddr.getNode();
+    if (!LoadAddrNode || !LoadAddrNode->isMachineOpcode() ||
+        LoadAddrNode->getMachineOpcode() != SLOW32::LOAD_ADDR)
+      return SDValue();
+
+    SDValue SymbolOp = LoadAddrNode->getOperand(0);
+    if (auto *GA = dyn_cast<GlobalAddressSDNode>(SymbolOp.getNode()))
+      return createLoadAddrForGlobal(DL, GA, ExtraOffset);
+
+    return SDValue();
+  }
+
+public:
   // Select address mode for loads/stores
   // This is critical for proper byte-wise memcpy expansion
   bool SelectAddr(SDValue N, SDValue &Base, SDValue &Offset) {
     SDLoc DL(N);
-    
-    // Handle GlobalAddress nodes with offsets (critical for memcpy expansion)
-    // This handles patterns like .L__const.main.str+1, .L__const.main.str+2, etc.
-    if (GlobalAddressSDNode *GA = dyn_cast<GlobalAddressSDNode>(N)) {
-      int64_t GOffset = GA->getOffset();
-      
-      // Split the GlobalAddress into base address and offset
-      // Create a GlobalAddress with offset 0 for the base
-      SDValue GABase = CurDAG->getTargetGlobalAddress(
-        GA->getGlobal(), DL, MVT::i32, 0, GA->getTargetFlags());
-      
-      // Generate LUI+ADDI to materialize the base address
-      SDValue Hi = SDValue(CurDAG->getMachineNode(SLOW32::LUI, DL, MVT::i32,
-                                                  GABase), 0);
-      Base = SDValue(CurDAG->getMachineNode(SLOW32::ADDI, DL, MVT::i32,
-                                            Hi, GABase), 0);
-      
-      // The offset from the GlobalAddress
-      Offset = CurDAG->getTargetConstant(GOffset, DL, MVT::i32);
+
+    if (auto *GA = dyn_cast<GlobalAddressSDNode>(N)) {
+      Base = createLoadAddrForGlobal(DL, GA, /*AdditionalOffset=*/0);
+      Offset = CurDAG->getTargetConstant(0, DL, MVT::i32);
       return true;
     }
-    
-    // Handle (base + constant) addressing
+
     if (N.getOpcode() == ISD::ADD) {
-      // Check if the first operand is a GlobalAddress with an offset
-      if (GlobalAddressSDNode *GA = dyn_cast<GlobalAddressSDNode>(N.getOperand(0))) {
-        if (auto *CN = dyn_cast<ConstantSDNode>(N.getOperand(1))) {
-          int64_t CVal = CN->getSExtValue();
-          int64_t GOffset = GA->getOffset();
-          int64_t TotalOffset = GOffset + CVal;
-          
-          // Check if total offset fits in 16-bit signed immediate
-          if (isInt<16>(TotalOffset)) {
-            // Create base address (GlobalAddress with offset 0)
-            SDValue GABase = CurDAG->getTargetGlobalAddress(
-              GA->getGlobal(), DL, MVT::i32, 0, GA->getTargetFlags());
-            
-            // Generate LUI+ADDI to materialize the base address
-            SDValue Hi = SDValue(CurDAG->getMachineNode(SLOW32::LUI, DL, MVT::i32,
-                                                        GABase), 0);
-            Base = SDValue(CurDAG->getMachineNode(SLOW32::ADDI, DL, MVT::i32,
-                                                  Hi, GABase), 0);
-            
-            Offset = CurDAG->getTargetConstant(TotalOffset, DL, MVT::i32);
-            return true;
-          }
+      if (auto *GA = dyn_cast<GlobalAddressSDNode>(N.getOperand(0))) {
+        if (const auto *CN = dyn_cast<ConstantSDNode>(N.getOperand(1))) {
+          Base = createLoadAddrForGlobal(DL, GA, CN->getSExtValue());
+          Offset = CurDAG->getTargetConstant(0, DL, MVT::i32);
+          return true;
         }
       }
-      
-      // Normal ADD pattern with constant offset
-      if (auto *CN = dyn_cast<ConstantSDNode>(N.getOperand(1))) {
+      if (auto *GA = dyn_cast<GlobalAddressSDNode>(N.getOperand(1))) {
+        if (const auto *CN = dyn_cast<ConstantSDNode>(N.getOperand(0))) {
+          Base = createLoadAddrForGlobal(DL, GA, CN->getSExtValue());
+          Offset = CurDAG->getTargetConstant(0, DL, MVT::i32);
+          return true;
+        }
+      }
+
+      if (const auto *CN = dyn_cast<ConstantSDNode>(N.getOperand(1))) {
+        if (SDValue Folded = tryFoldLoadAddrAddend(N.getOperand(0),
+                                                   CN->getSExtValue(), DL)) {
+          Base = Folded;
+          Offset = CurDAG->getTargetConstant(0, DL, MVT::i32);
+          return true;
+        }
+
         int64_t CVal = CN->getSExtValue();
-        // Check if constant fits in 16-bit signed immediate
-        if (isInt<16>(CVal)) {
+        if (isInt<12>(CVal)) {
           Base = N.getOperand(0);
           Offset = CurDAG->getTargetConstant(CVal, DL, MVT::i32);
           return true;
         }
       }
-    }
-    
-    // Handle nested ADD for (HI+LO)+offset patterns
-    if (N.getOpcode() == ISD::ADD && N.getOperand(0).getOpcode() == ISD::ADD) {
-      if (auto *CN = dyn_cast<ConstantSDNode>(N.getOperand(1))) {
+
+      if (const auto *CN = dyn_cast<ConstantSDNode>(N.getOperand(0))) {
+        if (SDValue Folded = tryFoldLoadAddrAddend(N.getOperand(1),
+                                                   CN->getSExtValue(), DL)) {
+          Base = Folded;
+          Offset = CurDAG->getTargetConstant(0, DL, MVT::i32);
+          return true;
+        }
+
         int64_t CVal = CN->getSExtValue();
-        if (isInt<16>(CVal)) {
-          Base = N.getOperand(0);
+        if (isInt<12>(CVal)) {
+          Base = N.getOperand(1);
           Offset = CurDAG->getTargetConstant(CVal, DL, MVT::i32);
           return true;
         }
       }
     }
-    
-    // Handle FrameIndex nodes
+
     if (auto *FI = dyn_cast<FrameIndexSDNode>(N)) {
       Base = CurDAG->getTargetFrameIndex(FI->getIndex(), MVT::i32);
       Offset = CurDAG->getTargetConstant(0, DL, MVT::i32);
       return true;
     }
-    
-    // Default case: base with 0 offset
+
     Base = N;
     Offset = CurDAG->getTargetConstant(0, DL, MVT::i32);
     return true;
@@ -122,6 +131,45 @@ public:
     }
     
     switch (N->getOpcode()) {
+    case ISD::ADD: {
+      SDLoc DL(N);
+      SDValue Op0 = N->getOperand(0);
+      SDValue Op1 = N->getOperand(1);
+
+      if (auto *GA = dyn_cast<GlobalAddressSDNode>(Op0)) {
+        if (const auto *CN = dyn_cast<ConstantSDNode>(Op1)) {
+          SDValue Folded =
+              createLoadAddrForGlobal(DL, GA, CN->getSExtValue());
+          ReplaceNode(N, Folded.getNode());
+          return;
+        }
+      }
+      if (auto *GA = dyn_cast<GlobalAddressSDNode>(Op1)) {
+        if (const auto *CN = dyn_cast<ConstantSDNode>(Op0)) {
+          SDValue Folded =
+              createLoadAddrForGlobal(DL, GA, CN->getSExtValue());
+          ReplaceNode(N, Folded.getNode());
+          return;
+        }
+      }
+
+      if (const auto *CN = dyn_cast<ConstantSDNode>(Op1)) {
+        if (SDValue Folded =
+                tryFoldLoadAddrAddend(Op0, CN->getSExtValue(), DL)) {
+          ReplaceNode(N, Folded.getNode());
+          return;
+        }
+      }
+      if (const auto *CN = dyn_cast<ConstantSDNode>(Op0)) {
+        if (SDValue Folded =
+                tryFoldLoadAddrAddend(Op1, CN->getSExtValue(), DL)) {
+          ReplaceNode(N, Folded.getNode());
+          return;
+        }
+      }
+
+      break;
+    }
     case ISD::Constant: {
       // Handle large constants that don't fit in immediate fields
       auto *CN = cast<ConstantSDNode>(N);
@@ -132,40 +180,35 @@ public:
         break;
       }
       
-      // For large constants, we need LUI + ADDI (RISC-V style)
+      // For large constants, we need LUI + ADDI (RISC-V style rounding)
       SDLoc DL(N);
       EVT VT = N->getValueType(0);
+
+      int64_t Hi = (Imm + 0x800) >> 12;
+      int32_t Lo = Imm - static_cast<int64_t>(Hi << 12);
+      assert(isInt<12>(Lo) && "Lo12 out of range after materialisation");
+
       SDValue Result;
-      
-      // Split into 20-bit upper and 12-bit lower parts
-      uint32_t UImm = (uint32_t)Imm;
-      int32_t Lo12 = UImm & 0xFFF;
-      int32_t Hi20 = (UImm >> 12) & 0xFFFFF;
-      
-      // If bit 11 of Lo12 is set, it will be sign-extended by ADDI
-      // We need to adjust Hi20 to compensate
-      if (Lo12 & 0x800) {
-        Hi20 = (Hi20 + 1) & 0xFFFFF;  // Increment and mask to 20 bits
-        // Keep Lo12 as the original 12-bit value for the instruction encoding
-        // ADDI will sign-extend it at runtime
-      }
-      
-      if (Hi20 == 0 && !(Lo12 & 0x800)) {
-        // Just ADDI is enough (small positive constant that fits in 12 bits)
+
+      uint32_t HiBits = static_cast<uint32_t>(Hi) & 0xFFFFF;
+      SDValue HiImm = CurDAG->getTargetConstant(HiBits, DL, MVT::i32);
+      APInt LoAP(32, static_cast<uint64_t>(static_cast<int64_t>(Lo)), true);
+      SDValue LoImm = CurDAG->getTargetConstant(LoAP, DL, MVT::i32);
+
+      if (Hi == 0) {
+        // Pure ADDI from zero (covers both positive and negative 12-bit values)
         Result = SDValue(CurDAG->getMachineNode(SLOW32::ADDI, DL, VT,
                                        CurDAG->getRegister(SLOW32::R0, VT),
-                                       CurDAG->getTargetConstant(Lo12, DL, VT)), 0);
-      } else if (Lo12 == 0) {
-        // Just LUI is enough (lower 12 bits are zero)
-        Result = SDValue(CurDAG->getMachineNode(SLOW32::LUI, DL, VT,
-                                       CurDAG->getTargetConstant(Hi20, DL, VT)), 0);
+                                       LoImm),
+                         0);
+      } else if (Lo == 0) {
+        Result = SDValue(
+            CurDAG->getMachineNode(SLOW32::LUI, DL, VT, HiImm), 0);
       } else {
-        // Need both LUI and ADDI
-        SDValue HiNode = SDValue(CurDAG->getMachineNode(SLOW32::LUI, DL, VT,
-                                    CurDAG->getTargetConstant(Hi20, DL, VT)), 0);
-        Result = SDValue(CurDAG->getMachineNode(SLOW32::ADDI, DL, VT,
-                                       HiNode,
-                                       CurDAG->getTargetConstant(Lo12, DL, VT)), 0);
+        SDValue HiNode =
+            SDValue(CurDAG->getMachineNode(SLOW32::LUI, DL, VT, HiImm), 0);
+        Result = SDValue(
+            CurDAG->getMachineNode(SLOW32::ADDI, DL, VT, HiNode, LoImm), 0);
       }
       
       ReplaceNode(N, Result.getNode());
diff --git a/llvm/lib/Target/SLOW32/SLOW32ISelLowering.cpp b/llvm/lib/Target/SLOW32/SLOW32ISelLowering.cpp
index 136206944..9dce7c471 100644
--- a/llvm/lib/Target/SLOW32/SLOW32ISelLowering.cpp
+++ b/llvm/lib/Target/SLOW32/SLOW32ISelLowering.cpp
@@ -9,7 +9,6 @@
 #include "llvm/CodeGen/MachineFrameInfo.h"
 #include "llvm/IR/Intrinsics.h"
 #include "llvm/Support/Debug.h"
-#include "llvm/ADT/StringSwitch.h"
 
 using namespace llvm;
 
@@ -28,6 +27,7 @@ SLOW32TargetLowering::SLOW32TargetLowering(const TargetMachine &TM)
   const SLOW32Subtarget &STI = static_cast<const SLOW32TargetMachine&>(TM).getSubtarget();
   computeRegisterProperties(STI.getRegisterInfo());
 
+
   // Old flag/glue family → expand away
   setOperationAction(ISD::ADDC, MVT::i32, Expand);
   setOperationAction(ISD::ADDE, MVT::i32, Expand);
@@ -80,16 +80,21 @@ SLOW32TargetLowering::SLOW32TargetLowering(const TargetMachine &TM)
   setOperationAction(ISD::UREM, MVT::i64, Expand);
   
   // SLOW32 is not recognized in LLVM's RuntimeLibcalls infrastructure,
-  // so we need to explicitly set which libcall implementations to use for i64 div/rem
-  // These use the standard compiler-rt/libgcc implementations
+  // so we need to explicitly set which libcall implementations to use for the
+  // division/rem library hooks we rely on.  These map to the
+  // compiler-rt/libgcc style helpers that our crt ships.
   setLibcallImpl(RTLIB::SDIV_I64, RTLIB::impl___divdi3);
   setLibcallImpl(RTLIB::UDIV_I64, RTLIB::impl___udivdi3);
   setLibcallImpl(RTLIB::SREM_I64, RTLIB::impl___moddi3);
   setLibcallImpl(RTLIB::UREM_I64, RTLIB::impl___umoddi3);
-  
-  // Set the calling convention for these libcall implementations to use C calling convention
+
+  setLibcallImpl(RTLIB::UDIV_I32, RTLIB::impl___udivsi3);
+  setLibcallImpl(RTLIB::UREM_I32, RTLIB::impl___umodsi3);
+
+  // All of these helpers follow the plain C calling convention.
   for (RTLIB::LibcallImpl Impl : {RTLIB::impl___divdi3, RTLIB::impl___udivdi3,
-                                   RTLIB::impl___moddi3, RTLIB::impl___umoddi3}) {
+                                  RTLIB::impl___moddi3, RTLIB::impl___umoddi3,
+                                  RTLIB::impl___udivsi3, RTLIB::impl___umodsi3}) {
     setLibcallImplCallingConv(Impl, CallingConv::C);
   }
   // These will use our shift-parts implementation
@@ -242,21 +247,12 @@ SDValue SLOW32TargetLowering::LowerGlobalAddress(SDValue Op, SelectionDAG &DAG)
 
   // Use %hi/%lo addressing for all global addresses (both functions and data)
   // Create target global address nodes with appropriate flags
-  SDValue TargetGAHi = DAG.getTargetGlobalAddress(GV, DL, VT,
-                                                   Offset, SLOW32II::MO_HI);
-  SDValue TargetGALo = DAG.getTargetGlobalAddress(GV, DL, VT,
-                                                   Offset, SLOW32II::MO_LO);
-
-  // Generate: lui rd, %hi(symbol)
-  SDValue Hi = DAG.getNode(SLOW32ISD::HI, DL, VT, TargetGAHi);
-
-  // Generate: addi rd, rd, %lo(symbol)
-  // Use ADD to get ADDI instruction with sign-extended 12-bit immediate
-  // This properly handles the sign extension and bit 11 adjustment
-  SDValue Lo = DAG.getNode(SLOW32ISD::LO, DL, VT, TargetGALo);
-
-  // Combine with ADD to generate ADDI (sign-extended LO12)
-  return DAG.getNode(ISD::ADD, DL, VT, Hi, Lo);
+  // Materialise the address through the LOAD_ADDR pseudo so we expand to
+  // canonical LUI/ADDI sequences post-RA.
+  SDValue TargetGA = DAG.getTargetGlobalAddress(GV, DL, VT, Offset,
+                                                GA->getTargetFlags());
+  SDNode *Mov = DAG.getMachineNode(SLOW32::LOAD_ADDR, DL, VT, TargetGA);
+  return SDValue(Mov, 0);
 }
 
 SDValue SLOW32TargetLowering::LowerLOAD(SDValue Op, SelectionDAG &DAG) const {
@@ -306,9 +302,11 @@ SDValue SLOW32TargetLowering::LowerExternalSymbol(SDValue Op, SelectionDAG &DAG)
     return SDValue();
   }
 
-  // For function calls, just return the target symbol - JAL will handle it directly
-  // For data references, we'd use %hi/%lo but that's rare for external symbols
-  return DAG.getTargetExternalSymbol(Symbol, VT);
+  // Materialise external addresses through LOAD_ADDR like regular globals so
+  // we benefit from the same folding and post-RA expansion.
+  SDValue TargetES = DAG.getTargetExternalSymbol(Symbol, VT, ES->getTargetFlags());
+  SDNode *Mov = DAG.getMachineNode(SLOW32::LOAD_ADDR, DL, VT, TargetES);
+  return SDValue(Mov, 0);
 }
 
 SDValue SLOW32TargetLowering::LowerJumpTable(SDValue Op, SelectionDAG &DAG) const {
@@ -317,88 +315,10 @@ SDValue SLOW32TargetLowering::LowerJumpTable(SDValue Op, SelectionDAG &DAG) cons
   EVT VT = Op.getValueType();
 
   // Similar to global addresses, use %hi/%lo addressing with RISC-V style
-  SDValue TargetJTHi = DAG.getTargetJumpTable(JT->getIndex(), VT, SLOW32II::MO_HI);
-  SDValue TargetJTLo = DAG.getTargetJumpTable(JT->getIndex(), VT, SLOW32II::MO_LO);
-
-  // Generate: lui rd, %hi(jumptable)
-  SDValue Hi = DAG.getNode(SLOW32ISD::HI, DL, VT, TargetJTHi);
-
-  // Generate: addi rd, rd, %lo(jumptable)
-  SDValue Lo = DAG.getNode(SLOW32ISD::LO, DL, VT, TargetJTLo);
-
-  // Combine with ADD to generate ADDI (sign-extended LO12)
-  return DAG.getNode(ISD::ADD, DL, VT, Hi, Lo);
-}
-
-SDValue SLOW32TargetLowering::LowerVASTART(SDValue Op, SelectionDAG &DAG) const {
-  MachineFunction &MF = DAG.getMachineFunction();
-  SLOW32MachineFunctionInfo *FuncInfo = MF.getInfo<SLOW32MachineFunctionInfo>();
-
-  SDLoc DL(Op);
-  SDValue FI = DAG.getFrameIndex(FuncInfo->getVarArgsFrameIndex(),
-                                 getPointerTy(MF.getDataLayout()));
-
-  // vastart just stores the address of the VarArgsFrameIndex slot into the
-  // memory location argument.
-  const Value *SV = cast<SrcValueSDNode>(Op.getOperand(2))->getValue();
-  return DAG.getStore(Op.getOperand(0), DL, FI, Op.getOperand(1),
-                      MachinePointerInfo(SV));
-}
-
-SDValue SLOW32TargetLowering::LowerVAARG(SDValue Op, SelectionDAG &DAG) const {
-  SDLoc DL(Op);
-  EVT RetVT = Op.getValueType();        // type the caller requested (i8/i16/i32/i64/ptr)
-  SDValue Chain = Op.getOperand(0);
-  SDValue VAListPtr = Op.getOperand(1); // i32* pointing to current AP
-
-  const DataLayout &DLay = DAG.getDataLayout();
-  EVT PtrVT = getPointerTy(DLay);
-
-  // 1) Load current AP from *va_list
-  //    (the VASTART implementation stored the address of the varargs area here)
-  MachinePointerInfo MPIList; // unknown for now; fine for stack/locals
-  SDValue AP = DAG.getLoad(PtrVT, DL, Chain, VAListPtr, MPIList, Align(4));
-  Chain = AP.getValue(1);
-
-  // 2) Compute the size to fetch from the slot according to default promotions
-  unsigned FetchBytes = 4;         // default for all ≤32-bit integer/pointer types
-  bool IsI64 = (RetVT == MVT::i64);
-  if (IsI64) FetchBytes = 8;
-
-  // 3) Perform the load(s)
-  SDValue Val;
-  if (IsI64) {
-    // Load lo/hi 32-bit words and build i64
-    SDValue Lo = DAG.getLoad(MVT::i32, DL, Chain, AP, MPIList, Align(4));
-    Chain = Lo.getValue(1);
-
-    SDValue APPlus4 = DAG.getNode(ISD::ADD, DL, PtrVT, AP,
-                                   DAG.getConstant(4, DL, PtrVT));
-    SDValue Hi = DAG.getLoad(MVT::i32, DL, Chain, APPlus4, MPIList, Align(4));
-    Chain = Hi.getValue(1);
-
-    // BUILD_PAIR {lo, hi} → i64 (legalizer will split if needed)
-    Val = DAG.getNode(ISD::BUILD_PAIR, DL, MVT::i64, Lo, Hi);
-  } else {
-    // Always load the full 32-bit promoted slot
-    SDValue W = DAG.getLoad(MVT::i32, DL, Chain, AP, MPIList, Align(4));
-    Chain = W.getValue(1);
-
-    if (RetVT == MVT::i32 || RetVT == PtrVT)
-      Val = W;
-    else if (RetVT == MVT::i16 || RetVT == MVT::i8)
-      Val = DAG.getNode(ISD::TRUNCATE, DL, RetVT, W);
-    else
-      return SDValue(); // unsupported (no FP in SLOW32); could Expand or libcall
-  }
-
-  // 4) Bump AP and store it back into *va_list
-  SDValue Bump = DAG.getConstant(FetchBytes, DL, PtrVT);
-  SDValue NextAP = DAG.getNode(ISD::ADD, DL, PtrVT, AP, Bump);
-  Chain = DAG.getStore(Chain, DL, NextAP, VAListPtr, MPIList, Align(4));
-
-  // 5) Return (value, chain)
-  return DAG.getMergeValues({Val, Chain}, DL);
+  SDValue TargetJT =
+      DAG.getTargetJumpTable(JT->getIndex(), VT, JT->getTargetFlags());
+  SDNode *Mov = DAG.getMachineNode(SLOW32::LOAD_ADDR, DL, VT, TargetJT);
+  return SDValue(Mov, 0);
 }
 
 SDValue SLOW32TargetLowering::LowerSELECT(SDValue Op, SelectionDAG &DAG) const {
@@ -684,6 +604,17 @@ SDValue SLOW32TargetLowering::LowerFormalArguments(
   MachineRegisterInfo &RegInfo = MF.getRegInfo();
   SLOW32MachineFunctionInfo *FuncInfo = MF.getInfo<SLOW32MachineFunctionInfo>();
 
+  // Make sure the prologue can freely touch the canonical stack/frame/link
+  // registers without tripping MachineVerifier: seed them as function and
+  // entry-block live-ins up front, mirroring what backends like RISC-V do.
+  MachineBasicBlock &EntryMBB = MF.front();
+  for (MCRegister R : {SLOW32::R29, SLOW32::R30, SLOW32::R31}) {
+    if (!RegInfo.isLiveIn(R))
+      RegInfo.addLiveIn(R);
+    if (!EntryMBB.isLiveIn(R))
+      EntryMBB.addLiveIn(R);
+  }
+
   // Arguments are passed in r3-r10 (a0-a7)
   static const unsigned ArgRegs[] = {
     SLOW32::R3, SLOW32::R4, SLOW32::R5, SLOW32::R6,
@@ -724,96 +655,44 @@ SDValue SLOW32TargetLowering::LowerFormalArguments(
 
   // Handle varargs
   if (isVarArg) {
-    // Save any unused argument registers to the varargs save area
-    int VarArgsSaveSize = 4 * (NumArgRegs - UsedArgRegs);
+    SmallVector<SDValue, 8> StoreChains;
     int VarArgsFrameIndex;
+    int VarArgsSaveSize = 4 * (NumArgRegs - UsedArgRegs);
 
     if (VarArgsSaveSize == 0) {
-      // All registers were used, varargs start on stack
+      // All register slots consumed; first variable argument lives on the stack
       VarArgsFrameIndex = MFI.CreateFixedObject(4, StackOffset, true);
     } else {
-      // Create save area for unused argument registers
-      VarArgsFrameIndex = MFI.CreateFixedObject(VarArgsSaveSize, -VarArgsSaveSize, true);
-
-      // Save the unused argument registers
-      // IMPORTANT: Save them in sequential memory order for va_arg to work
+      // Materialise a contiguous stack area for the remaining argument registers
+      VarArgsFrameIndex =
+          MFI.CreateFixedObject(VarArgsSaveSize, -VarArgsSaveSize, true);
       SDValue FIN = DAG.getFrameIndex(VarArgsFrameIndex, MVT::i32);
 
-      // Hardcode the stores to ensure correct order (debugging the reordering issue)
-      // For sum(count, ...) with count in r3, we need to save r4-r10
-      if (UsedArgRegs == 1) {
-        // Save r4 at offset 0
-        unsigned VReg4 = RegInfo.createVirtualRegister(&SLOW32::GPRRegClass);
-        RegInfo.addLiveIn(SLOW32::R4, VReg4);
-        SDValue Val4 = DAG.getCopyFromReg(Chain, dl, VReg4, MVT::i32);
-        Chain = DAG.getStore(Chain, dl, Val4, FIN, MachinePointerInfo());
-
-        // Save r5 at offset 4
-        unsigned VReg5 = RegInfo.createVirtualRegister(&SLOW32::GPRRegClass);
-        RegInfo.addLiveIn(SLOW32::R5, VReg5);
-        SDValue Val5 = DAG.getCopyFromReg(Chain, dl, VReg5, MVT::i32);
-        SDValue Addr5 = DAG.getNode(ISD::ADD, dl, MVT::i32, FIN,
-                                    DAG.getConstant(4, dl, MVT::i32));
-        Chain = DAG.getStore(Chain, dl, Val5, Addr5, MachinePointerInfo());
-
-        // Save r6 at offset 8
-        unsigned VReg6 = RegInfo.createVirtualRegister(&SLOW32::GPRRegClass);
-        RegInfo.addLiveIn(SLOW32::R6, VReg6);
-        SDValue Val6 = DAG.getCopyFromReg(Chain, dl, VReg6, MVT::i32);
-        SDValue Addr6 = DAG.getNode(ISD::ADD, dl, MVT::i32, FIN,
-                                    DAG.getConstant(8, dl, MVT::i32));
-        Chain = DAG.getStore(Chain, dl, Val6, Addr6, MachinePointerInfo());
-
-        // Save r7-r10 similarly
-        unsigned VReg7 = RegInfo.createVirtualRegister(&SLOW32::GPRRegClass);
-        RegInfo.addLiveIn(SLOW32::R7, VReg7);
-        SDValue Val7 = DAG.getCopyFromReg(Chain, dl, VReg7, MVT::i32);
-        SDValue Addr7 = DAG.getNode(ISD::ADD, dl, MVT::i32, FIN,
-                                    DAG.getConstant(12, dl, MVT::i32));
-        Chain = DAG.getStore(Chain, dl, Val7, Addr7, MachinePointerInfo());
-
-        unsigned VReg8 = RegInfo.createVirtualRegister(&SLOW32::GPRRegClass);
-        RegInfo.addLiveIn(SLOW32::R8, VReg8);
-        SDValue Val8 = DAG.getCopyFromReg(Chain, dl, VReg8, MVT::i32);
-        SDValue Addr8 = DAG.getNode(ISD::ADD, dl, MVT::i32, FIN,
-                                    DAG.getConstant(16, dl, MVT::i32));
-        Chain = DAG.getStore(Chain, dl, Val8, Addr8, MachinePointerInfo());
-
-        unsigned VReg9 = RegInfo.createVirtualRegister(&SLOW32::GPRRegClass);
-        RegInfo.addLiveIn(SLOW32::R9, VReg9);
-        SDValue Val9 = DAG.getCopyFromReg(Chain, dl, VReg9, MVT::i32);
-        SDValue Addr9 = DAG.getNode(ISD::ADD, dl, MVT::i32, FIN,
-                                    DAG.getConstant(20, dl, MVT::i32));
-        Chain = DAG.getStore(Chain, dl, Val9, Addr9, MachinePointerInfo());
-
-        unsigned VReg10 = RegInfo.createVirtualRegister(&SLOW32::GPRRegClass);
-        RegInfo.addLiveIn(SLOW32::R10, VReg10);
-        SDValue Val10 = DAG.getCopyFromReg(Chain, dl, VReg10, MVT::i32);
-        SDValue Addr10 = DAG.getNode(ISD::ADD, dl, MVT::i32, FIN,
-                                     DAG.getConstant(24, dl, MVT::i32));
-        Chain = DAG.getStore(Chain, dl, Val10, Addr10, MachinePointerInfo());
-      } else {
-        // Generic loop for other cases - properly chain everything
-        for (unsigned i = UsedArgRegs; i < NumArgRegs; ++i) {
-          unsigned VReg = RegInfo.createVirtualRegister(&SLOW32::GPRRegClass);
-          RegInfo.addLiveIn(ArgRegs[i], VReg);
-
-          // Get value from register
-          SDValue ArgValue = DAG.getCopyFromReg(Chain, dl, VReg, MVT::i32);
-
-          // Calculate address
-          int Offset = (i - UsedArgRegs) * 4;
-          SDValue Addr = Offset == 0 ? FIN :
-                         DAG.getNode(ISD::ADD, dl, MVT::i32, FIN,
-                                    DAG.getConstant(Offset, dl, MVT::i32));
-
-          // Store and update chain
-          Chain = DAG.getStore(Chain, dl, ArgValue, Addr, MachinePointerInfo());
-        }
+      for (unsigned I = UsedArgRegs; I < NumArgRegs; ++I) {
+        unsigned VReg = RegInfo.createVirtualRegister(&SLOW32::GPRRegClass);
+        RegInfo.addLiveIn(ArgRegs[I], VReg);
+
+        SDValue ArgValue = DAG.getCopyFromReg(Chain, dl, VReg, MVT::i32);
+        int Offset = static_cast<int>((I - UsedArgRegs) * 4);
+
+        SDValue Addr =
+            Offset == 0
+                ? FIN
+                : DAG.getNode(ISD::ADD, dl, MVT::i32, FIN,
+                              DAG.getConstant(Offset, dl, MVT::i32));
+
+        SDValue Store = DAG.getStore(
+            Chain, dl, ArgValue, Addr,
+            MachinePointerInfo::getFixedStack(MF, VarArgsFrameIndex, Offset));
+        StoreChains.push_back(Store);
       }
     }
 
-    // Record the varargs frame index
+    if (!StoreChains.empty()) {
+      StoreChains.push_back(Chain);
+      Chain = DAG.getNode(ISD::TokenFactor, dl, MVT::Other, StoreChains);
+    }
+
     FuncInfo->setVarArgsFrameIndex(VarArgsFrameIndex);
     FuncInfo->setVarArgsSaveSize(VarArgsSaveSize);
   }
@@ -904,7 +783,7 @@ SDValue SLOW32TargetLowering::LowerCall(TargetLowering::CallLoweringInfo &CLI,
   Chain = DAG.getCALLSEQ_START(Chain, StackSize, 0, dl);
 
   // Get stack pointer for storing arguments
-  SDValue StackPtr = DAG.getRegister(SLOW32::SP, MVT::i32);
+  SDValue StackPtr = DAG.getRegister(SLOW32::R29, MVT::i32);
 
   // Push stack arguments (arguments beyond the first 8)
   for (unsigned i = 8; i < OutVals.size(); ++i) {
@@ -1011,7 +890,7 @@ bool SLOW32TargetLowering::isLegalAddressingMode(const DataLayout &DL,
                                                  const AddrMode &AM,
                                                  Type *Ty, unsigned AS,
                                                  Instruction *I) const {
-  // SLOW32 supports [base + simm16] addressing mode
+  // SLOW32 supports [base + simm12] addressing mode
   // No scaled index, no complex addressing
 
   // Check if offset fits in 16-bit signed immediate
@@ -1058,34 +937,7 @@ SDValue SLOW32TargetLowering::LowerROTL(SDValue Op, SelectionDAG &DAG) const {
 }
 
 SDValue SLOW32TargetLowering::LowerConstant(SDValue Op, SelectionDAG &DAG) const {
-  SDLoc DL(Op);
-  ConstantSDNode *CN = cast<ConstantSDNode>(Op);
-  EVT VT = Op.getValueType();
-  int64_t Val = CN->getSExtValue();
-
-  // For small constants, just return the constant node
-  if (Val >= -32768 && Val <= 65535) {
-    return Op;  // Let normal patterns handle it
-  }
-
-  // For large constants, we need LUI + ORI
-  // Split into high and low parts
-  int32_t Hi = (Val >> 16) & 0xFFFF;
-  int32_t Lo = Val & 0xFFFF;
-
-  // LUI loads upper 16 bits
-  SDValue HiVal = DAG.getConstant(Hi, DL, VT);
-  SDValue LUI = DAG.getNode(ISD::SHL, DL, VT, HiVal,
-                            DAG.getConstant(16, DL, VT));
-
-  // If low part is zero, we're done
-  if (Lo == 0) {
-    return LUI;
-  }
-
-  // Otherwise OR in the low bits
-  SDValue LoVal = DAG.getConstant(Lo, DL, VT);
-  return DAG.getNode(ISD::OR, DL, VT, LUI, LoVal);
+  return Op;
 }
 
 SDValue SLOW32TargetLowering::LowerROTR(SDValue Op, SelectionDAG &DAG) const {
@@ -1574,333 +1426,3 @@ SDValue SLOW32TargetLowering::LowerUREM(SDValue Op, SelectionDAG &DAG) const {
   SDValue Chain = DAG.getEntryNode();
   return makeLibCall(DAG, RTLIB::UREM_I32, MVT::i32, {L, R}, CallOptions, DL, Chain).first;
 }
-//===----------------------------------------------------------------------===//
-// Inline Assembly Support
-//===----------------------------------------------------------------------===//
-
-TargetLowering::ConstraintType
-SLOW32TargetLowering::getConstraintType(StringRef Constraint) const {
-  if (Constraint.size() == 1) {
-    switch (Constraint[0]) {
-    default:
-      break;
-    case 'r':
-      return C_RegisterClass;
-    case 'i':
-    case 'n':
-      return C_Immediate;
-    case 'm':
-      return C_Memory;
-    }
-  }
-  return TargetLowering::getConstraintType(Constraint);
-}
-
-std::pair<unsigned, const TargetRegisterClass *>
-SLOW32TargetLowering::getRegForInlineAsmConstraint(
-    const TargetRegisterInfo *TRI, StringRef Constraint, MVT VT) const {
-  // First handle single-character constraints
-  if (Constraint.size() == 1) {
-    switch (Constraint[0]) {
-    case 'r':
-      // General purpose register for any integer type
-      if (VT == MVT::i32 || VT == MVT::i16 || VT == MVT::i8 || VT == MVT::i1)
-        return std::make_pair(0U, &SLOW32::GPRRegClass);
-      break;
-    default:
-      break;
-    }
-  }
-  
-  // Handle specific register names like {r1}, {r2}, etc.
-  if (Constraint.size() > 1 && Constraint[0] == '{' && 
-      Constraint[Constraint.size() - 1] == '}') {
-    StringRef RegName = Constraint.slice(1, Constraint.size() - 1);
-    
-    // Try to parse as r0-r31
-    if (RegName.starts_with("r")) {
-      unsigned RegNum;
-      if (!RegName.substr(1).getAsInteger(10, RegNum) && RegNum < 32) {
-        // Map to the corresponding register
-        unsigned Reg = SLOW32::R0 + RegNum;
-        return std::make_pair(Reg, &SLOW32::GPRRegClass);
-      }
-    }
-    
-    // Handle register aliases
-    unsigned Reg = StringSwitch<unsigned>(RegName.lower())
-                      .Case("zero", SLOW32::R0)
-                      .Case("rv", SLOW32::R1)
-                      .Case("t0", SLOW32::R2)
-                      .Case("a0", SLOW32::R3)
-                      .Case("a1", SLOW32::R4)
-                      .Case("a2", SLOW32::R5)
-                      .Case("a3", SLOW32::R6)
-                      .Case("a4", SLOW32::R7)
-                      .Case("a5", SLOW32::R8)
-                      .Case("a6", SLOW32::R9)
-                      .Case("a7", SLOW32::R10)
-                      .Case("sp", SLOW32::R29)
-                      .Case("fp", SLOW32::R30)
-                      .Case("lr", SLOW32::R31)
-                      .Case("ra", SLOW32::R31)
-                      .Default(0);
-    
-    if (Reg)
-      return std::make_pair(Reg, &SLOW32::GPRRegClass);
-  }
-  
-  // Fall back to the default implementation
-  return TargetLowering::getRegForInlineAsmConstraint(TRI, Constraint, VT);
-}
-
-void SLOW32TargetLowering::LowerAsmOperandForConstraint(
-    SDValue Op, StringRef Constraint, std::vector<SDValue> &Ops,
-    SelectionDAG &DAG) const {
-  // Handle 'i' and 'n' immediate constraints
-  if (Constraint.size() == 1) {
-    switch (Constraint[0]) {
-    case 'i': // Simple integer immediate
-    case 'n': // Integer immediate for "n"umerics
-      if (ConstantSDNode *C = dyn_cast<ConstantSDNode>(Op)) {
-        Ops.push_back(DAG.getTargetConstant(C->getZExtValue(), SDLoc(Op),
-                                           Op.getValueType()));
-        return;
-      }
-      break;
-    default:
-      break;
-    }
-  }
-  
-  // Fall back to default implementation
-  TargetLowering::LowerAsmOperandForConstraint(Op, Constraint, Ops, DAG);
-}//===----------------------------------------------------------------------===//
-// Custom DAG Combine Implementation
-//===----------------------------------------------------------------------===//
-
-SDValue SLOW32TargetLowering::PerformDAGCombine(SDNode *N, 
-                                                DAGCombinerInfo &DCI) const {
-  switch (N->getOpcode()) {
-  default: break;
-  case ISD::ADD:   return performADDCombine(N, DCI);
-  case ISD::MUL:   return performMULCombine(N, DCI);
-  case ISD::AND:   return performANDCombine(N, DCI);
-  case ISD::OR:    return performORCombine(N, DCI);
-  case ISD::SHL:   return performSHLCombine(N, DCI);
-  case ISD::LOAD:  return performLOADCombine(N, DCI);
-  case ISD::STORE: return performSTORECombine(N, DCI);
-  }
-  return SDValue();
-}
-
-SDValue SLOW32TargetLowering::performADDCombine(SDNode *N, 
-                                                DAGCombinerInfo &DCI) const {
-  SelectionDAG &DAG = DCI.DAG;
-  SDLoc DL(N);
-  
-  // Combine (add x, (shl y, 1)) -> (add (shl y, 1), x) 
-  // to help with addressing mode matching
-  SDValue N0 = N->getOperand(0);
-  SDValue N1 = N->getOperand(1);
-  
-  // Only do add -> mul transforms BEFORE legalization
-  // to avoid infinite loops. The MUL combine does mul -> add+shl
-  // after legalization, so we must stop doing add+shl -> mul by then.
-  // We need to check both isAfterLegalizeDAG and !isBeforeLegalize
-  // to catch all post-legalization phases.
-  if (!DCI.isBeforeLegalize()) {
-    return SDValue();
-  }
-  
-  // Combine (add (shl x, 2), x) -> (mul x, 5)
-  // Useful for array indexing patterns
-  if (N0.getOpcode() == ISD::SHL && N0.getOperand(0) == N1) {
-    if (ConstantSDNode *C = dyn_cast<ConstantSDNode>(N0.getOperand(1))) {
-      unsigned ShAmt = C->getZExtValue();
-      if (ShAmt >= 1 && ShAmt <= 3) {
-        // (x << n) + x = x * ((1 << n) + 1)
-        unsigned MulConst = (1U << ShAmt) + 1;
-        return DAG.getNode(ISD::MUL, DL, N->getValueType(0), N1,
-                          DAG.getConstant(MulConst, DL, N->getValueType(0)));
-      }
-    }
-  }
-  
-  // Combine (add x, (shl x, 2)) -> (mul x, 5)
-  if (N1.getOpcode() == ISD::SHL && N1.getOperand(0) == N0) {
-    if (ConstantSDNode *C = dyn_cast<ConstantSDNode>(N1.getOperand(1))) {
-      unsigned ShAmt = C->getZExtValue();
-      if (ShAmt >= 1 && ShAmt <= 3) {
-        unsigned MulConst = (1U << ShAmt) + 1;
-        return DAG.getNode(ISD::MUL, DL, N->getValueType(0), N0,
-                          DAG.getConstant(MulConst, DL, N->getValueType(0)));
-      }
-    }
-  }
-  
-  return SDValue();
-}
-
-SDValue SLOW32TargetLowering::performMULCombine(SDNode *N, 
-                                                DAGCombinerInfo &DCI) const {
-  SelectionDAG &DAG = DCI.DAG;
-  SDLoc DL(N);
-  
-  // Optimize multiply by constants
-  ConstantSDNode *C = dyn_cast<ConstantSDNode>(N->getOperand(1));
-  if (!C) return SDValue();
-  
-  uint64_t MulAmt = C->getZExtValue();
-  
-  // Only do power-of-2 combines before legalization to avoid infinite loops
-  if (DCI.isBeforeLegalize() && isPowerOf2_64(MulAmt)) {
-    unsigned ShAmt = Log2_64(MulAmt);
-    return DAG.getNode(ISD::SHL, DL, N->getValueType(0),
-                      N->getOperand(0),
-                      DAG.getConstant(ShAmt, DL, MVT::i32));
-  }
-  
-  // Only do these non-power-of-2 combines AFTER legalization to avoid
-  // infinite loops with the ADD combine that does the reverse transformation.
-  // The ADD combine runs during legalization, converting add+shl -> mul.
-  // If we do mul -> add+shl before that, we get an infinite loop.
-  if (!DCI.isAfterLegalizeDAG()) {
-    return SDValue();
-  }
-  
-  // Optimize (mul x, 3) -> (add (shl x, 1), x)
-  // Optimize (mul x, 5) -> (add (shl x, 2), x)
-  // Optimize (mul x, 9) -> (add (shl x, 3), x)
-  if (MulAmt == 3 || MulAmt == 5 || MulAmt == 9) {
-    unsigned ShAmt = (MulAmt == 3) ? 1 : (MulAmt == 5) ? 2 : 3;
-    SDValue Shl = DAG.getNode(ISD::SHL, DL, N->getValueType(0),
-                             N->getOperand(0),
-                             DAG.getConstant(ShAmt, DL, MVT::i32));
-    return DAG.getNode(ISD::ADD, DL, N->getValueType(0), Shl, N->getOperand(0));
-  }
-  
-  // Optimize (mul x, 6) -> (shl (mul x, 3), 1) -> (shl (add (shl x, 1), x), 1)
-  if (MulAmt == 6) {
-    SDValue Shl1 = DAG.getNode(ISD::SHL, DL, N->getValueType(0),
-                               N->getOperand(0),
-                               DAG.getConstant(1, DL, MVT::i32));
-    SDValue Add = DAG.getNode(ISD::ADD, DL, N->getValueType(0), 
-                             Shl1, N->getOperand(0));
-    return DAG.getNode(ISD::SHL, DL, N->getValueType(0),
-                      Add, DAG.getConstant(1, DL, MVT::i32));
-  }
-  
-  return SDValue();
-}
-
-SDValue SLOW32TargetLowering::performANDCombine(SDNode *N, 
-                                                DAGCombinerInfo &DCI) const {
-  SDLoc DL(N);
-  
-  // Optimize (and x, 0xFFFF) to zero-extend pattern if beneficial
-  ConstantSDNode *C = dyn_cast<ConstantSDNode>(N->getOperand(1));
-  if (!C) return SDValue();
-  
-  uint64_t AndMask = C->getZExtValue();
-  
-  // Optimize (and (srl x, 16), 0xFFFF) -> (srli x, 16)
-  // The shift already zero-extends the upper bits
-  if (AndMask == 0xFFFF && N->getOperand(0).getOpcode() == ISD::SRL) {
-    if (ConstantSDNode *ShC = dyn_cast<ConstantSDNode>(N->getOperand(0).getOperand(1))) {
-      if (ShC->getZExtValue() == 16) {
-        return N->getOperand(0); // The SRL already masks correctly
-      }
-    }
-  }
-  
-  // Optimize (and x, 1) when used by a branch - helps with bool tests
-  if (AndMask == 1 && N->hasOneUse()) {
-    SDNode *Use = N->use_begin()->getUser();
-    if (Use->getOpcode() == ISD::BRCOND || Use->getOpcode() == ISD::BR_CC) {
-      // Keep as-is, this is optimal for flag testing
-      return SDValue();
-    }
-  }
-  
-  return SDValue();
-}
-
-SDValue SLOW32TargetLowering::performORCombine(SDNode *N, 
-                                               DAGCombinerInfo &DCI) const {
-  SelectionDAG &DAG = DCI.DAG;
-  SDLoc DL(N);
-  
-  // Combine (or (shl x, 16), (and y, 0xFFFF)) -> pack high/low halves
-  // This pattern appears in 32-bit constant materialization
-  SDValue N0 = N->getOperand(0);
-  SDValue N1 = N->getOperand(1);
-  
-  if (N0.getOpcode() == ISD::SHL && N1.getOpcode() == ISD::AND) {
-    if (ConstantSDNode *ShC = dyn_cast<ConstantSDNode>(N0.getOperand(1))) {
-      if (ConstantSDNode *AndC = dyn_cast<ConstantSDNode>(N1.getOperand(1))) {
-        if (ShC->getZExtValue() == 16 && AndC->getZExtValue() == 0xFFFF) {
-          // This is a common pattern for packing two 16-bit values
-          // Keep it as-is for now, but could optimize further
-          return SDValue();
-        }
-      }
-    }
-  }
-  
-  return SDValue();
-}
-
-SDValue SLOW32TargetLowering::performSHLCombine(SDNode *N, 
-                                                DAGCombinerInfo &DCI) const {
-  SelectionDAG &DAG = DCI.DAG;
-  SDLoc DL(N);
-  
-  // Combine (shl (shl x, c1), c2) -> (shl x, c1+c2)
-  if (N->getOperand(0).getOpcode() == ISD::SHL) {
-    if (ConstantSDNode *C1 = dyn_cast<ConstantSDNode>(N->getOperand(0).getOperand(1))) {
-      if (ConstantSDNode *C2 = dyn_cast<ConstantSDNode>(N->getOperand(1))) {
-        unsigned ShAmt = C1->getZExtValue() + C2->getZExtValue();
-        if (ShAmt < 32) {
-          return DAG.getNode(ISD::SHL, DL, N->getValueType(0),
-                            N->getOperand(0).getOperand(0),
-                            DAG.getConstant(ShAmt, DL, MVT::i32));
-        }
-      }
-    }
-  }
-  
-  return SDValue();
-}
-
-SDValue SLOW32TargetLowering::performLOADCombine(SDNode *N, 
-                                                 DAGCombinerInfo &DCI) const {
-  LoadSDNode *LD = cast<LoadSDNode>(N);
-  
-  // Don't combine volatile loads
-  if (LD->isVolatile())
-    return SDValue();
-  
-  // Could add combines for:
-  // - Combining narrow loads into wider loads
-  // - Optimizing load+shift patterns
-  // - Pre-increment/post-increment addressing patterns
-  
-  return SDValue();
-}
-
-SDValue SLOW32TargetLowering::performSTORECombine(SDNode *N, 
-                                                  DAGCombinerInfo &DCI) const {
-  StoreSDNode *ST = cast<StoreSDNode>(N);
-  
-  // Don't combine volatile stores
-  if (ST->isVolatile())
-    return SDValue();
-  
-  // Could add combines for:
-  // - Combining narrow stores into wider stores
-  // - Optimizing store of constants
-  // - Pre-increment/post-increment addressing patterns
-  
-  return SDValue();
-}
diff --git a/llvm/lib/Target/SLOW32/SLOW32ISelLoweringCombines.cpp b/llvm/lib/Target/SLOW32/SLOW32ISelLoweringCombines.cpp
new file mode 100644
index 000000000..bbbc900d1
--- /dev/null
+++ b/llvm/lib/Target/SLOW32/SLOW32ISelLoweringCombines.cpp
@@ -0,0 +1,185 @@
+//===-- SLOW32ISelLoweringCombines.cpp - DAG Combine Helpers -------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+#include "SLOW32ISelLowering.h"
+#include "llvm/CodeGen/SelectionDAG.h"
+
+using namespace llvm;
+
+#define DEBUG_TYPE "slow32-lower"
+
+SDValue SLOW32TargetLowering::PerformDAGCombine(SDNode *N,
+                                                DAGCombinerInfo &DCI) const {
+  switch (N->getOpcode()) {
+  default:
+    break;
+  case ISD::ADD:
+    return performADDCombine(N, DCI);
+  case ISD::MUL:
+    return performMULCombine(N, DCI);
+  case ISD::AND:
+    return performANDCombine(N, DCI);
+  case ISD::OR:
+    return performORCombine(N, DCI);
+  case ISD::SHL:
+    return performSHLCombine(N, DCI);
+  case ISD::LOAD:
+    return performLOADCombine(N, DCI);
+  case ISD::STORE:
+    return performSTORECombine(N, DCI);
+  }
+  return SDValue();
+}
+
+SDValue SLOW32TargetLowering::performADDCombine(SDNode *N,
+                                                DAGCombinerInfo &DCI) const {
+  SelectionDAG &DAG = DCI.DAG;
+  SDLoc DL(N);
+
+  SDValue N0 = N->getOperand(0);
+  SDValue N1 = N->getOperand(1);
+
+  if (!DCI.isBeforeLegalize())
+    return SDValue();
+
+  if (N0.getOpcode() == ISD::SHL && N0.getOperand(0) == N1) {
+    if (auto *C = dyn_cast<ConstantSDNode>(N0.getOperand(1))) {
+      unsigned ShAmt = C->getZExtValue();
+      if (ShAmt >= 1 && ShAmt <= 3) {
+        unsigned MulConst = (1U << ShAmt) + 1;
+        return DAG.getNode(ISD::MUL, DL, N->getValueType(0), N1,
+                           DAG.getConstant(MulConst, DL, N->getValueType(0)));
+      }
+    }
+  }
+
+  if (N1.getOpcode() == ISD::SHL && N1.getOperand(0) == N0) {
+    if (auto *C = dyn_cast<ConstantSDNode>(N1.getOperand(1))) {
+      unsigned ShAmt = C->getZExtValue();
+      if (ShAmt >= 1 && ShAmt <= 3) {
+        unsigned MulConst = (1U << ShAmt) + 1;
+        return DAG.getNode(ISD::MUL, DL, N->getValueType(0), N0,
+                           DAG.getConstant(MulConst, DL, N->getValueType(0)));
+      }
+    }
+  }
+
+  return SDValue();
+}
+
+SDValue SLOW32TargetLowering::performMULCombine(SDNode *N,
+                                                DAGCombinerInfo &DCI) const {
+  SelectionDAG &DAG = DCI.DAG;
+  SDLoc DL(N);
+
+  auto *C = dyn_cast<ConstantSDNode>(N->getOperand(1));
+  if (!C)
+    return SDValue();
+
+  uint64_t MulAmt = C->getZExtValue();
+
+  if (DCI.isBeforeLegalize() && isPowerOf2_64(MulAmt)) {
+    unsigned ShAmt = Log2_64(MulAmt);
+    return DAG.getNode(ISD::SHL, DL, N->getValueType(0), N->getOperand(0),
+                       DAG.getConstant(ShAmt, DL, MVT::i32));
+  }
+
+  if (!DCI.isAfterLegalizeDAG())
+    return SDValue();
+
+  if (MulAmt == 3 || MulAmt == 5 || MulAmt == 9) {
+    unsigned ShAmt = (MulAmt == 3) ? 1 : (MulAmt == 5) ? 2 : 3;
+    SDValue Shl = DAG.getNode(ISD::SHL, DL, N->getValueType(0), N->getOperand(0),
+                              DAG.getConstant(ShAmt, DL, MVT::i32));
+    return DAG.getNode(ISD::ADD, DL, N->getValueType(0), Shl, N->getOperand(0));
+  }
+
+  if (MulAmt == 6) {
+    SDValue Shl1 = DAG.getNode(ISD::SHL, DL, N->getValueType(0), N->getOperand(0),
+                               DAG.getConstant(1, DL, MVT::i32));
+    SDValue Add = DAG.getNode(ISD::ADD, DL, N->getValueType(0), Shl1,
+                              N->getOperand(0));
+    return DAG.getNode(ISD::SHL, DL, N->getValueType(0), Add,
+                       DAG.getConstant(1, DL, MVT::i32));
+  }
+
+  return SDValue();
+}
+
+SDValue SLOW32TargetLowering::performANDCombine(SDNode *N,
+                                                DAGCombinerInfo &DCI) const {
+  auto *C = dyn_cast<ConstantSDNode>(N->getOperand(1));
+  if (!C)
+    return SDValue();
+
+  uint64_t AndMask = C->getZExtValue();
+
+  if (AndMask == 0xFFFF && N->getOperand(0).getOpcode() == ISD::SRL) {
+    if (auto *ShC = dyn_cast<ConstantSDNode>(N->getOperand(0).getOperand(1)))
+      if (ShC->getZExtValue() == 16)
+        return N->getOperand(0);
+  }
+
+  if (AndMask == 1 && N->hasOneUse()) {
+    SDNode *Use = N->use_begin()->getUser();
+    if (Use->getOpcode() == ISD::BRCOND || Use->getOpcode() == ISD::BR_CC)
+      return SDValue();
+  }
+
+  return SDValue();
+}
+
+SDValue SLOW32TargetLowering::performORCombine(SDNode *N,
+                                               DAGCombinerInfo &DCI) const {
+  SDValue N0 = N->getOperand(0);
+  SDValue N1 = N->getOperand(1);
+
+  if (N0.getOpcode() == ISD::SHL && N1.getOpcode() == ISD::AND) {
+    if (auto *ShC = dyn_cast<ConstantSDNode>(N0.getOperand(1)))
+      if (auto *AndC = dyn_cast<ConstantSDNode>(N1.getOperand(1)))
+        if (ShC->getZExtValue() == 16 && AndC->getZExtValue() == 0xFFFF)
+          return SDValue();
+  }
+
+  return SDValue();
+}
+
+SDValue SLOW32TargetLowering::performSHLCombine(SDNode *N,
+                                                DAGCombinerInfo &DCI) const {
+  SelectionDAG &DAG = DCI.DAG;
+  SDLoc DL(N);
+
+  if (N->getOperand(0).getOpcode() == ISD::SHL) {
+    if (auto *C1 = dyn_cast<ConstantSDNode>(N->getOperand(0).getOperand(1)))
+      if (auto *C2 = dyn_cast<ConstantSDNode>(N->getOperand(1))) {
+        unsigned ShAmt = C1->getZExtValue() + C2->getZExtValue();
+        if (ShAmt < 32)
+          return DAG.getNode(ISD::SHL, DL, N->getValueType(0),
+                             N->getOperand(0).getOperand(0),
+                             DAG.getConstant(ShAmt, DL, MVT::i32));
+      }
+  }
+
+  return SDValue();
+}
+
+SDValue SLOW32TargetLowering::performLOADCombine(SDNode *N,
+                                                 DAGCombinerInfo &DCI) const {
+  auto *LD = cast<LoadSDNode>(N);
+  if (LD->isVolatile())
+    return SDValue();
+  return SDValue();
+}
+
+SDValue SLOW32TargetLowering::performSTORECombine(SDNode *N,
+                                                  DAGCombinerInfo &DCI) const {
+  auto *ST = cast<StoreSDNode>(N);
+  if (ST->isVolatile())
+    return SDValue();
+  return SDValue();
+}
diff --git a/llvm/lib/Target/SLOW32/SLOW32ISelLoweringInlineAsm.cpp b/llvm/lib/Target/SLOW32/SLOW32ISelLoweringInlineAsm.cpp
new file mode 100644
index 000000000..246cc322a
--- /dev/null
+++ b/llvm/lib/Target/SLOW32/SLOW32ISelLoweringInlineAsm.cpp
@@ -0,0 +1,122 @@
+//===-- SLOW32ISelLoweringInlineAsm.cpp - Inline Asm Helpers -------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+#include "SLOW32ISelLowering.h"
+#include "SLOW32.h"
+#include "SLOW32RegisterInfo.h"
+#include "llvm/ADT/StringSwitch.h"
+#include "llvm/Support/MathExtras.h"
+
+using namespace llvm;
+
+#define DEBUG_TYPE "slow32-lower"
+
+TargetLowering::ConstraintType
+SLOW32TargetLowering::getConstraintType(StringRef Constraint) const {
+  if (Constraint.size() == 1) {
+    switch (Constraint[0]) {
+    default:
+      break;
+    case 'r':
+      return C_RegisterClass;
+    case 'i':
+    case 'n':
+      return C_Immediate;
+    case 'm':
+      return C_Memory;
+    }
+  }
+  return TargetLowering::getConstraintType(Constraint);
+}
+
+std::pair<unsigned, const TargetRegisterClass *>
+SLOW32TargetLowering::getRegForInlineAsmConstraint(
+    const TargetRegisterInfo *TRI, StringRef Constraint, MVT VT) const {
+  const TargetRegisterClass *GPRRC =
+      TRI ? TRI->getRegClass(SLOW32::GPRRegClassID) : nullptr;
+
+  // First handle single-character constraints
+  if (Constraint.size() == 1) {
+    switch (Constraint[0]) {
+    case 'r':
+      // General purpose register for any integer type
+      if (GPRRC && (VT.isInteger() || VT == MVT::Other))
+        return std::make_pair(0U, GPRRC);
+      break;
+    default:
+      break;
+    }
+  }
+
+  // Handle specific register names like {r1}, {r2}, etc.
+  if (Constraint.size() > 1 && Constraint[0] == '{' &&
+      Constraint.back() == '}') {
+    StringRef RegName = Constraint.slice(1, Constraint.size() - 1);
+
+    // Try to parse as r0-r31
+    if (RegName.starts_with("r")) {
+      unsigned RegNum;
+      if (!RegName.substr(1).getAsInteger(10, RegNum) && RegNum < 32) {
+        // Map to the corresponding register
+        unsigned Reg = SLOW32::R0 + RegNum;
+        if (GPRRC)
+          return std::make_pair(Reg, GPRRC);
+      }
+    }
+
+    // Handle register aliases
+    unsigned Reg = StringSwitch<unsigned>(RegName.lower())
+                      .Case("zero", SLOW32::R0)
+                      .Case("rv", SLOW32::R1)
+                      .Case("t0", SLOW32::R2)
+                      .Case("a0", SLOW32::R3)
+                      .Case("a1", SLOW32::R4)
+                      .Case("a2", SLOW32::R5)
+                      .Case("a3", SLOW32::R6)
+                      .Case("a4", SLOW32::R7)
+                      .Case("a5", SLOW32::R8)
+                      .Case("a6", SLOW32::R9)
+                      .Case("a7", SLOW32::R10)
+                      .Case("sp", SLOW32::R29)
+                      .Case("fp", SLOW32::R30)
+                      .Case("lr", SLOW32::R31)
+                      .Case("ra", SLOW32::R31)
+                      .Default(0);
+
+    if (Reg && GPRRC)
+      return std::make_pair(Reg, GPRRC);
+  }
+
+  // Fall back to the default implementation
+  return TargetLowering::getRegForInlineAsmConstraint(TRI, Constraint, VT);
+}
+
+void SLOW32TargetLowering::LowerAsmOperandForConstraint(
+    SDValue Op, StringRef Constraint, std::vector<SDValue> &Ops,
+    SelectionDAG &DAG) const {
+  // Handle 'i' and 'n' immediate constraints
+  if (Constraint.size() == 1) {
+    switch (Constraint[0]) {
+    case 'i': // Simple integer immediate
+    case 'n': // Integer immediate for "n"umerics
+      if (ConstantSDNode *C = dyn_cast<ConstantSDNode>(Op)) {
+        int64_t Value = C->getSExtValue();
+        if (isInt<12>(Value)) {
+          Ops.push_back(DAG.getTargetConstant(Value, SDLoc(Op), MVT::i32));
+          return;
+        }
+      }
+      break;
+    default:
+      break;
+    }
+  }
+
+  // Fall back to default implementation
+  TargetLowering::LowerAsmOperandForConstraint(Op, Constraint, Ops, DAG);
+}
diff --git a/llvm/lib/Target/SLOW32/SLOW32ISelLoweringVarargs.cpp b/llvm/lib/Target/SLOW32/SLOW32ISelLoweringVarargs.cpp
new file mode 100644
index 000000000..652d413f4
--- /dev/null
+++ b/llvm/lib/Target/SLOW32/SLOW32ISelLoweringVarargs.cpp
@@ -0,0 +1,101 @@
+//===-- SLOW32ISelLoweringVarargs.cpp - Varargs Lowering -----------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+#include "SLOW32ISelLowering.h"
+#include "SLOW32MachineFunctionInfo.h"
+#include "llvm/CodeGen/MachineFunction.h"
+#include "llvm/CodeGen/SelectionDAG.h"
+#include "llvm/Support/Alignment.h"
+
+using namespace llvm;
+
+#define DEBUG_TYPE "slow32-lower"
+
+SDValue SLOW32TargetLowering::LowerVASTART(SDValue Op,
+                                           SelectionDAG &DAG) const {
+  MachineFunction &MF = DAG.getMachineFunction();
+  SLOW32MachineFunctionInfo *FuncInfo = MF.getInfo<SLOW32MachineFunctionInfo>();
+
+  SDLoc DL(Op);
+  SDValue FI = DAG.getFrameIndex(FuncInfo->getVarArgsFrameIndex(),
+                                 getPointerTy(MF.getDataLayout()));
+
+  // vastart just stores the address of the VarArgsFrameIndex slot into the
+  // memory location argument.
+  const Value *SV = cast<SrcValueSDNode>(Op.getOperand(2))->getValue();
+  return DAG.getStore(Op.getOperand(0), DL, FI, Op.getOperand(1),
+                      MachinePointerInfo(SV));
+}
+
+SDValue SLOW32TargetLowering::LowerVAARG(SDValue Op,
+                                         SelectionDAG &DAG) const {
+  SDLoc DL(Op);
+  EVT RetVT = Op.getValueType();        // type the caller requested (i8/i16/i32/i64/ptr)
+  SDValue Chain = Op.getOperand(0);
+  SDValue VAListPtr = Op.getOperand(1); // i32* pointing to current AP
+
+  const DataLayout &DLay = DAG.getDataLayout();
+  EVT PtrVT = getPointerTy(DLay);
+
+  // Bail out to the generic expander for types we do not explicitly support yet
+  // (e.g. floating-point). This keeps the compiler from asserting until
+  // SLOW32 grows the required ABI support.
+  bool IsPtr = (RetVT == PtrVT);
+  bool IsI64 = (RetVT == MVT::i64);
+  bool IsSmallInt = RetVT.isInteger() && RetVT.getSizeInBits() <= 32 && RetVT != MVT::i64;
+
+  if (!IsI64 && !IsPtr && !IsSmallInt) {
+    SDValue Expanded = DAG.expandVAArg(Op.getNode());
+    return DAG.getMergeValues({Expanded, Expanded.getValue(1)}, DL);
+  }
+
+  // 1) Load current AP from *va_list
+  //    (the VASTART implementation stored the address of the varargs area here)
+  MachinePointerInfo MPIList; // unknown for now; fine for stack/locals
+  SDValue AP = DAG.getLoad(PtrVT, DL, Chain, VAListPtr, MPIList, Align(4));
+  Chain = AP.getValue(1);
+
+  // 2) Compute the size to fetch from the slot according to default promotions
+  unsigned FetchBytes = 4;         // default for all ≤32-bit integer/pointer types
+  if (IsI64)
+    FetchBytes = 8;
+
+  // 3) Perform the load(s)
+  SDValue Val;
+  if (IsI64) {
+    // Load lo/hi 32-bit words and build i64
+    SDValue Lo = DAG.getLoad(MVT::i32, DL, Chain, AP, MPIList, Align(4));
+    Chain = Lo.getValue(1);
+
+    SDValue APPlus4 = DAG.getNode(ISD::ADD, DL, PtrVT, AP,
+                                   DAG.getConstant(4, DL, PtrVT));
+    SDValue Hi = DAG.getLoad(MVT::i32, DL, Chain, APPlus4, MPIList, Align(4));
+    Chain = Hi.getValue(1);
+
+    // BUILD_PAIR {lo, hi} → i64 (legalizer will split if needed)
+    Val = DAG.getNode(ISD::BUILD_PAIR, DL, MVT::i64, Lo, Hi);
+  } else {
+    // Always load the full 32-bit promoted slot
+    SDValue W = DAG.getLoad(MVT::i32, DL, Chain, AP, MPIList, Align(4));
+    Chain = W.getValue(1);
+
+    if (RetVT == MVT::i32 || IsPtr)
+      Val = W;
+    else if (RetVT.getSizeInBits() < 32)
+      Val = DAG.getNode(ISD::TRUNCATE, DL, RetVT, W);
+  }
+
+  // 4) Bump AP and store it back into *va_list
+  SDValue Bump = DAG.getConstant(FetchBytes, DL, PtrVT);
+  SDValue NextAP = DAG.getNode(ISD::ADD, DL, PtrVT, AP, Bump);
+  Chain = DAG.getStore(Chain, DL, NextAP, VAListPtr, MPIList, Align(4));
+
+  // 5) Return (value, chain)
+  return DAG.getMergeValues({Val, Chain}, DL);
+}
+
diff --git a/llvm/lib/Target/SLOW32/SLOW32InstrInfo.cpp b/llvm/lib/Target/SLOW32/SLOW32InstrInfo.cpp
index 4107c9524..8ee7cac65 100644
--- a/llvm/lib/Target/SLOW32/SLOW32InstrInfo.cpp
+++ b/llvm/lib/Target/SLOW32/SLOW32InstrInfo.cpp
@@ -9,9 +9,10 @@
 #include "SLOW32InstrInfo.h"
 #include "SLOW32.h"
 #include "SLOW32Subtarget.h"
+#include "llvm/ADT/SmallVector.h"
 #include "llvm/CodeGen/MachineInstrBuilder.h"
-#include "llvm/Support/ErrorHandling.h"
 #include "llvm/Support/Debug.h"
+#include "llvm/Support/ErrorHandling.h"
 
 #define DEBUG_TYPE "slow32-branch"
 
@@ -22,6 +23,47 @@
 
 using namespace llvm;
 
+namespace {
+
+bool isConditionalBranch(unsigned Opcode) {
+  switch (Opcode) {
+  case SLOW32::BEQ_pat:
+  case SLOW32::BNE_pat:
+  case SLOW32::BLT:
+  case SLOW32::BGE:
+    return true;
+  default:
+    return false;
+  }
+}
+
+MachineBasicBlock *getBranchTarget(const MachineInstr &MI) {
+  assert(MI.getDesc().isBranch() && "Unexpected non-branch opcode");
+  unsigned Idx = MI.getNumExplicitOperands();
+  if (Idx == 0)
+    return nullptr;
+  const MachineOperand &MO = MI.getOperand(Idx - 1);
+  // NOTE: SLOW32 conditional branch pseudos still use an immediate slot for
+  // their offset. Earlier revs blindly called getMBB() here which returned
+  // garbage and triggered TailDuplicator asserts when the optimizer tried to
+  // rewrite CFG edges. Keep this guard in place (and update tests) if we ever
+  // adjust the TableGen patterns.
+  if (!MO.isMBB())
+    return nullptr;
+  return MO.getMBB();
+}
+
+void buildCondFromBranch(MachineInstr &MI,
+                         SmallVectorImpl<MachineOperand> &Cond) {
+  assert(isConditionalBranch(MI.getOpcode()) &&
+         "Trying to build a condition from a non-conditional branch");
+  Cond.push_back(MachineOperand::CreateImm(MI.getOpcode()));
+  Cond.push_back(MI.getOperand(0));
+  Cond.push_back(MI.getOperand(1));
+}
+
+} // namespace
+
 SLOW32InstrInfo::SLOW32InstrInfo(const SLOW32Subtarget &STI)
     : SLOW32GenInstrInfo(STI, SLOW32::ADJCALLSTACKDOWN, SLOW32::ADJCALLSTACKUP,
                          /*CatchRetOpcode=*/0, /*ReturnOpcode=*/0),
@@ -76,254 +118,161 @@ void SLOW32InstrInfo::loadRegFromStackSlot(MachineBasicBlock &MBB,
 
 // Analyze the branching code at the end of MBB
 bool SLOW32InstrInfo::analyzeBranch(MachineBasicBlock &MBB,
-                                     MachineBasicBlock *&TBB,
-                                     MachineBasicBlock *&FBB,
-                                     SmallVectorImpl<MachineOperand> &Cond,
-                                     bool AllowModify) const {
-  // Always start clean - reset outputs to avoid stale data
+                                    MachineBasicBlock *&TBB,
+                                    MachineBasicBlock *&FBB,
+                                    SmallVectorImpl<MachineOperand> &Cond,
+                                    bool AllowModify) const {
   TBB = nullptr;
   FBB = nullptr;
   Cond.clear();
-  
-  LLVM_DEBUG(dbgs() << "analyzeBranch BB#" << MBB.getNumber() 
-                    << " allowModify=" << (AllowModify ? "yes" : "no")
-                    << " successors=" << MBB.succ_size() << "\n");
-  
-  // Start from the end of the basic block and look for terminators
+
   MachineBasicBlock::iterator I = MBB.getLastNonDebugInstr();
   if (I == MBB.end())
-    return false; // empty block => fallthrough
-  
-  // IMPORTANT: If the block has no successors, we can't analyze it meaningfully
-  // This can happen after noreturn calls (exit, abort, etc.)
-  if (MBB.succ_empty())
-    return true; // Can't analyze blocks with no successors
-  
-  // Skip back over any non-terminator instructions to find the actual terminators
-  // This handles blocks where calls or other instructions come before the branch
-  while (I != MBB.begin() && !isUnpredicatedTerminator(*I)) {
-    LLVM_DEBUG(dbgs() << "  Skipping non-terminator: " << *I);
+    return false;
+
+  while (I->isCFIInstruction() && I != MBB.begin())
     --I;
-  }
-    
-  // If we didn't find any terminator, this block falls through
-  if (!isUnpredicatedTerminator(*I)) {
-    LLVM_DEBUG(dbgs() << "  No terminator found, treating as fallthrough\n");
-    return false; // No terminator => fallthrough
-  }
-  
-  LLVM_DEBUG(dbgs() << "  Last non-debug instr opcode: " << I->getOpcode() 
-                    << " (BEQ_pat=" << SLOW32::BEQ_pat << ", BNE_pat=" << SLOW32::BNE_pat
-                    << ", BR=" << SLOW32::BR << ")\n");
-
-  // If the block ends with a return, there is no analyzable branch.
-  // BUT: TailDup may have left stale branches immediately before the RET.
-  // If we're allowed to modify, strip them here so Folder/Placement don't loop.
-  // Note: Don't bail out on BR (which has isBarrier=1) - it's a branch we need to analyze!
-  if (I->isReturn()) {
-    if (AllowModify) {
-      int BytesRemoved = 0;
-      unsigned N = removeBranch(MBB, &BytesRemoved);
-      LLVM_DEBUG(dbgs() << "  end=RET; removed " << N << " stale branch(es)\n");
-    } else {
-      LLVM_DEBUG(dbgs() << "  end=RET; no modify, leaving as-is\n");
-    }
-    return false;  // Successfully analyzed: no branches (just return)
+
+  if (!isUnpredicatedTerminator(*I))
+    return false;
+
+  MachineBasicBlock::iterator FirstUncond = MBB.end();
+  int NumTerminators = 0;
+  for (auto It = I.getReverse(); It != MBB.rend() && isUnpredicatedTerminator(*It);
+       ++It) {
+    ++NumTerminators;
+    if (It->getDesc().isUnconditionalBranch() ||
+        It->getDesc().isIndirectBranch())
+      FirstUncond = It.getReverse();
   }
 
-  // Check for unconditional branch (BR)
-  if (I->getOpcode() == SLOW32::BR) {
-    LLVM_DEBUG(dbgs() << "  Last instr is BR\n");
-    if (!I->getOperand(0).isMBB())
-      return true; // Can't handle indirect branch
-    
-    // Check if there's a conditional branch before this unconditional branch
-    if (I != MBB.begin()) {
-      MachineBasicBlock::iterator J = I;
-      --J;
-      while ((J->isDebugInstr() || J->isCFIInstruction()) && J != MBB.begin())
-        --J;
-        
-      unsigned PrevOpcode = J->getOpcode();
-      if (PrevOpcode == SLOW32::BEQ_pat || PrevOpcode == SLOW32::BNE_pat ||
-          PrevOpcode == SLOW32::BLT || PrevOpcode == SLOW32::BGE) {
-        // Conditional branch followed by unconditional branch
-        LLVM_DEBUG(dbgs() << "  Found cond before uncond, opcode=" << PrevOpcode << "\n");
-        if (J->getOperand(2).isMBB()) {
-          TBB = J->getOperand(2).getMBB(); // Conditional target
-          FBB = I->getOperand(0).getMBB(); // Unconditional target
-          
-          // Save the condition
-          Cond.push_back(MachineOperand::CreateImm(PrevOpcode));
-          Cond.push_back(J->getOperand(0)); // First register
-          Cond.push_back(J->getOperand(1)); // Second register
-          return false;
-        }
-      }
+  if (AllowModify && FirstUncond != MBB.end()) {
+    while (std::next(FirstUncond) != MBB.end()) {
+      std::next(FirstUncond)->eraseFromParent();
+      --NumTerminators;
     }
-    
-    // Just an unconditional branch
-    TBB = I->getOperand(0).getMBB();
-    return false;
+    I = FirstUncond;
   }
 
-  // Check for conditional branches
-  unsigned Opcode = I->getOpcode();
-  if (Opcode == SLOW32::BEQ_pat || Opcode == SLOW32::BNE_pat ||
-      Opcode == SLOW32::BLT || Opcode == SLOW32::BGE) {
-    LLVM_DEBUG(dbgs() << "  Last instr is conditional branch opcode=" << Opcode << "\n");
+  if (I->getDesc().isIndirectBranch() || I->isPreISelOpcode())
+    return true;
 
+  if (NumTerminators > 2)
+    return true;
 
-    // Conditional branch
-    MachineBasicBlock *TargetBB = nullptr;
-    if (I->getOperand(2).isMBB()) {
-      TargetBB = I->getOperand(2).getMBB();
-    } else {
-      return true; // Can't handle this branch
+  if (NumTerminators == 1) {
+    if (I->getDesc().isUnconditionalBranch()) {
+      if (MachineBasicBlock *Target = getBranchTarget(*I)) {
+        TBB = Target;
+        return false;
+      }
+      return true;
     }
-
-    // Check if there's a following unconditional branch
-    MachineBasicBlock::iterator J = I;
-    ++J;
-    while (J != MBB.end() && (J->isDebugInstr() || J->isCFIInstruction()))
-      ++J;
-    
-    if (J != MBB.end() && J->getOpcode() == SLOW32::BR && J->getOperand(0).isMBB()) {
-      // Conditional branch followed by unconditional branch
-      TBB = TargetBB;
-      FBB = J->getOperand(0).getMBB();
-      
-      // Save the condition
-      Cond.push_back(MachineOperand::CreateImm(Opcode));
-      Cond.push_back(I->getOperand(0)); // First register
-      Cond.push_back(I->getOperand(1)); // Second register
-      
-      LLVM_DEBUG(dbgs() << "  Found cond+uncond: TBB=BB#" << TBB->getNumber() 
-                        << " FBB=BB#" << (FBB ? FBB->getNumber() : -1) << "\n");
+    if (I->getDesc().isConditionalBranch()) {
+      MachineBasicBlock *Target = getBranchTarget(*I);
+      if (!Target)
+        return true;
+      buildCondFromBranch(*I, Cond);
+      TBB = Target;
       return false;
     }
-    
-    // Just a conditional branch with fall-through
-    TBB = TargetBB;
-    
-    // CRITICAL: Check if this block actually has a fallthrough successor
-    // A conditional branch should have 2 successors (taken and fallthrough)
-    // If it only has 1 successor, the fallthrough path is unreachable/deleted
-    if (MBB.succ_size() == 1) {
-      LLVM_DEBUG(dbgs() << "  Conditional branch with only 1 successor - no fallthrough!\n");
-      // This is really an unconditional branch disguised as conditional
-      // Return true to indicate we can't properly analyze this
-      return true;
-    }
-    
-    // Don't normalize conditional branches that target layout successor
-    // (they still need the explicit branch even if target is next block)
-    LLVM_DEBUG(dbgs() << "  Found cond only: TBB=BB#" << (TBB ? TBB->getNumber() : -1) << "\n");
-    
-    // Save the condition
-    Cond.push_back(MachineOperand::CreateImm(Opcode));
-    Cond.push_back(I->getOperand(0)); // First register
-    Cond.push_back(I->getOperand(1)); // Second register
-    
-    return false;
+    return true;
   }
 
-  // If we get here, we have an unrecognized terminator instruction
-  LLVM_DEBUG(dbgs() << "  Unrecognized terminator instruction (opcode=" << I->getOpcode() << ")\n");
-  return true; // Can't analyze this terminator
+  // NumTerminators == 2
+  MachineBasicBlock::iterator SecondLast = std::prev(I);
+  while (SecondLast->isCFIInstruction() || SecondLast->isDebugInstr()) {
+    if (SecondLast == MBB.begin())
+      break;
+    --SecondLast;
+  }
+
+  if (!SecondLast->getDesc().isConditionalBranch() ||
+      !I->getDesc().isUnconditionalBranch())
+    return true;
+
+  MachineBasicBlock *TrueTarget = getBranchTarget(*SecondLast);
+  MachineBasicBlock *FalseTarget = getBranchTarget(*I);
+  if (!TrueTarget || !FalseTarget)
+    return true;
+
+  buildCondFromBranch(*SecondLast, Cond);
+  TBB = TrueTarget;
+  FBB = FalseTarget;
+  return false;
 }
 
 unsigned SLOW32InstrInfo::insertBranch(MachineBasicBlock &MBB,
-                                        MachineBasicBlock *TBB,
-                                        MachineBasicBlock *FBB,
-                                        ArrayRef<MachineOperand> Cond,
-                                        const DebugLoc &DL,
-                                        int *BytesAdded) const {
+                                       MachineBasicBlock *TBB,
+                                       MachineBasicBlock *FBB,
+                                       ArrayRef<MachineOperand> Cond,
+                                       const DebugLoc &DL,
+                                       int *BytesAdded) const {
   assert(TBB && "insertBranch must have a target basic block");
-  
-  unsigned Count = 0;
-  
-  // Always insert at the end of the block to maintain proper order
-  // Branch Folder expects us to build the terminator sequence from scratch
-  
-  // Unconditional branch
+
+  if (BytesAdded)
+    *BytesAdded = 0;
+
   if (Cond.empty()) {
-    // Only insert if TBB is not the layout successor (fallthrough)
-    if (!MBB.isLayoutSuccessor(TBB)) {
-      BuildMI(&MBB, DL, get(SLOW32::BR)).addMBB(TBB);
-      Count = 1;
-    }
-    // Note: Don't add successors here - Branch Folder manages CFG edges
-  } else {
-    // Conditional branch
-    assert(Cond.size() == 3 && "Invalid condition");
-    unsigned Opcode = Cond[0].getImm();
-    
-    // Emit conditional branch first
-    BuildMI(&MBB, DL, get(Opcode))
-      .add(Cond[1])  // First register
-      .add(Cond[2])  // Second register
-      .addMBB(TBB);
-    Count = 1;
-    
-    // Add unconditional branch if FBB is specified and it's not fallthrough
-    // This comes AFTER the conditional branch
-    if (FBB && !MBB.isLayoutSuccessor(FBB)) {
-      BuildMI(&MBB, DL, get(SLOW32::BR)).addMBB(FBB);
-      Count = 2;
-    }
-    // Note: Don't add successors here - Branch Folder manages CFG edges
+    MachineInstr &MI = *BuildMI(&MBB, DL, get(SLOW32::BR)).addMBB(TBB);
+    if (BytesAdded)
+      *BytesAdded += getInstSizeInBytes(MI);
+    return 1;
   }
-  
+
+  assert(Cond.size() == 3 && "Invalid condition");
+
+  MachineInstr &CondMI =
+      *BuildMI(&MBB, DL, get(Cond[0].getImm()))
+            .add(Cond[1])
+            .add(Cond[2])
+            .addMBB(TBB);
   if (BytesAdded)
-    *BytesAdded = Count * 4; // Each instruction is 4 bytes
-    
-  return Count;
+    *BytesAdded += getInstSizeInBytes(CondMI);
+
+  if (!FBB)
+    return 1;
+
+  MachineInstr &BrMI = *BuildMI(&MBB, DL, get(SLOW32::BR)).addMBB(FBB);
+  if (BytesAdded)
+    *BytesAdded += getInstSizeInBytes(BrMI);
+  return 2;
 }
 
 unsigned SLOW32InstrInfo::removeBranch(MachineBasicBlock &MBB,
-                                        int *BytesRemoved) const {
-  unsigned Count = 0;
-  
-  // Helper to check if an instruction is a branch
-  auto IsBranch = [](unsigned Opc) {
-    return Opc == SLOW32::BR ||
-           Opc == SLOW32::BEQ_pat ||
-           Opc == SLOW32::BNE_pat ||
-           Opc == SLOW32::BLT ||
-           Opc == SLOW32::BGE;
-  };
-  
-  // Robust approach: repeatedly get last non-debug instruction and remove if branch
-  // This avoids iterator invalidation issues
-  while (true) {
-    MachineBasicBlock::iterator I = MBB.getLastNonDebugInstr();
-    if (I == MBB.end())
-      break;
-    
-    // Walk back over trailing CFI
-    while (I != MBB.begin() && I->isCFIInstruction())
-      --I;
-    
-    if (!IsBranch(I->getOpcode()))
-      break;
-    
-    I->eraseFromParent();
-    ++Count;
-  }
-  
-  // Note: Don't modify successors here - Branch Folder manages CFG edges
-  
+                                       int *BytesRemoved) const {
   if (BytesRemoved)
-    *BytesRemoved = Count * 4;
-    
-  return Count;
+    *BytesRemoved = 0;
+
+  MachineBasicBlock::iterator I = MBB.getLastNonDebugInstr();
+  if (I == MBB.end())
+    return 0;
+
+  if (!I->getDesc().isUnconditionalBranch() &&
+      !I->getDesc().isConditionalBranch())
+    return 0;
+
+  if (BytesRemoved)
+    *BytesRemoved += getInstSizeInBytes(*I);
+  I->eraseFromParent();
+
+  I = MBB.getLastNonDebugInstr();
+  if (I == MBB.end())
+    return 1;
+
+  if (!I->getDesc().isConditionalBranch())
+    return 1;
+
+  if (BytesRemoved)
+    *BytesRemoved += getInstSizeInBytes(*I);
+  I->eraseFromParent();
+  return 2;
 }
 
 bool SLOW32InstrInfo::expandPostRAPseudo(MachineInstr &MI) const {
   switch (MI.getOpcode()) {
   case SLOW32::LOAD_ADDR: {
-    // Expand LOAD_ADDR pseudo into LUI + ORI sequence
+    // Expand LOAD_ADDR pseudo into LUI + ADDI sequence (RISC-V style)
     // This ensures they stay together in the correct order
     MachineBasicBlock &MBB = *MI.getParent();
     const DebugLoc &DL = MI.getDebugLoc();
@@ -331,14 +280,17 @@ bool SLOW32InstrInfo::expandPostRAPseudo(MachineInstr &MI) const {
     const MachineOperand &Symbol = MI.getOperand(1);
     
     // Generate LUI rd, %hi(symbol)
-    BuildMI(MBB, MI, DL, get(SLOW32::LUI), DestReg)
-      .add(Symbol);  // Will be printed with %hi
-    
-    // Generate ORI rd, rd, %lo(symbol)  
-    BuildMI(MBB, MI, DL, get(SLOW32::ORI), DestReg)
-      .addReg(DestReg)
-      .add(Symbol);  // Will be printed with %lo
-    
+    MachineInstrBuilder HiMI =
+        BuildMI(MBB, MI, DL, get(SLOW32::LUI), DestReg).add(Symbol);
+    HiMI->getOperand(1).setTargetFlags(SLOW32II::MO_HI);
+
+    // Generate ADDI rd, rd, %lo(symbol)  
+    MachineInstrBuilder LoMI =
+        BuildMI(MBB, MI, DL, get(SLOW32::ADDI), DestReg)
+            .addReg(DestReg)
+            .add(Symbol);
+    LoMI->getOperand(2).setTargetFlags(SLOW32II::MO_LO);
+
     MI.eraseFromParent();
     return true;
   }
@@ -390,4 +342,4 @@ bool SLOW32InstrInfo::reverseBranchCondition(
 //   
 //   // Don't duplicate blocks ending with return or barrier instructions
 //   return !(I->isReturn() || I->isBarrier());
-// }
\ No newline at end of file
+// }
diff --git a/llvm/lib/Target/SLOW32/SLOW32InstrInfo.td b/llvm/lib/Target/SLOW32/SLOW32InstrInfo.td
index 2903b7967..c3e6178da 100644
--- a/llvm/lib/Target/SLOW32/SLOW32InstrInfo.td
+++ b/llvm/lib/Target/SLOW32/SLOW32InstrInfo.td
@@ -8,13 +8,13 @@ def brtarget : Operand<OtherVT> {
   let PrintMethod = "printBranchTarget";
 }
 
-// Define immediate operands with proper constraints
-def simm16 : Operand<i32>, ImmLeaf<i32, [{
-  return isInt<16>(Imm);
+// Define immediate operands with proper constraints (RV-style 12-bit window)
+def simm12 : Operand<i32>, ImmLeaf<i32, [{
+  return isInt<12>(Imm);
 }]>;
 
-def uimm16 : Operand<i32>, ImmLeaf<i32, [{
-  return isUInt<16>(Imm);
+def uimm12 : Operand<i32>, ImmLeaf<i32, [{
+  return isUInt<12>(Imm);
 }]>;
 
 // Complex address pattern for load/store instructions
@@ -59,10 +59,10 @@ let hasSideEffects = 0, mayLoad = 0, mayStore = 0 in {
   def AND  : S32Inst<(outs GPR:$rd), (ins GPR:$rs1, GPR:$rs2), "and $rd, $rs1, $rs2">;
   def OR   : S32Inst<(outs GPR:$rd), (ins GPR:$rs1, GPR:$rs2), "or  $rd, $rs1, $rs2">;
   def XOR  : S32Inst<(outs GPR:$rd), (ins GPR:$rs1, GPR:$rs2), "xor $rd, $rs1, $rs2">;
-  def ADDI : S32Inst<(outs GPR:$rd), (ins GPR:$rs1, simm16:$imm), "addi $rd, $rs1, $imm">;
-  def ANDI : S32Inst<(outs GPR:$rd), (ins GPR:$rs1, uimm16:$imm), "andi $rd, $rs1, $imm">;
-  def ORI  : S32Inst<(outs GPR:$rd), (ins GPR:$rs1, uimm16:$imm), "ori  $rd, $rs1, $imm">;
-  def XORI : S32Inst<(outs GPR:$rd), (ins GPR:$rs1, uimm16:$imm), "xori $rd, $rs1, $imm">;
+  def ADDI : S32Inst<(outs GPR:$rd), (ins GPR:$rs1, simm12:$imm), "addi $rd, $rs1, $imm">;
+  def ANDI : S32Inst<(outs GPR:$rd), (ins GPR:$rs1, simm12:$imm), "andi $rd, $rs1, $imm">;
+  def ORI  : S32Inst<(outs GPR:$rd), (ins GPR:$rs1, simm12:$imm), "ori  $rd, $rs1, $imm">;
+  def XORI : S32Inst<(outs GPR:$rd), (ins GPR:$rs1, simm12:$imm), "xori $rd, $rs1, $imm">;
   // Register-register shifts
   def SLL  : S32Inst<(outs GPR:$rd), (ins GPR:$rs1, GPR:$rs2), "sll $rd, $rs1, $rs2">;
   def SRL  : S32Inst<(outs GPR:$rd), (ins GPR:$rs1, GPR:$rs2), "srl $rd, $rs1, $rs2">;
@@ -94,18 +94,18 @@ let hasSideEffects = 0, mayLoad = 0, mayStore = 0 in {
 
 // Load instructions
 let hasSideEffects = 0, mayLoad = 1, mayStore = 0 in {
-  def LDB  : S32Inst<(outs GPR:$rd), (ins GPR:$rs1, i32imm:$imm), "ldb $rd, $rs1+$imm">;
-  def LDBU : S32Inst<(outs GPR:$rd), (ins GPR:$rs1, i32imm:$imm), "ldbu $rd, $rs1+$imm">;
-  def LDH  : S32Inst<(outs GPR:$rd), (ins GPR:$rs1, i32imm:$imm), "ldh $rd, $rs1+$imm">;
-  def LDHU : S32Inst<(outs GPR:$rd), (ins GPR:$rs1, i32imm:$imm), "ldhu $rd, $rs1+$imm">;
-  def LDW  : S32Inst<(outs GPR:$rd), (ins GPR:$rs1, i32imm:$imm), "ldw $rd, $rs1+$imm">;
+  def LDB  : S32Inst<(outs GPR:$rd), (ins GPR:$rs1, simm12:$imm), "ldb $rd, $rs1+$imm">;
+  def LDBU : S32Inst<(outs GPR:$rd), (ins GPR:$rs1, simm12:$imm), "ldbu $rd, $rs1+$imm">;
+  def LDH  : S32Inst<(outs GPR:$rd), (ins GPR:$rs1, simm12:$imm), "ldh $rd, $rs1+$imm">;
+  def LDHU : S32Inst<(outs GPR:$rd), (ins GPR:$rs1, simm12:$imm), "ldhu $rd, $rs1+$imm">;
+  def LDW  : S32Inst<(outs GPR:$rd), (ins GPR:$rs1, simm12:$imm), "ldw $rd, $rs1+$imm">;
 }
 
 // Store instructions 
 let hasSideEffects = 0, mayLoad = 0, mayStore = 1 in {
-  def STB  : S32Inst<(outs), (ins GPR:$rs1, GPR:$rs2, i32imm:$imm), "stb $rs1+$imm, $rs2">;
-  def STH  : S32Inst<(outs), (ins GPR:$rs1, GPR:$rs2, i32imm:$imm), "sth $rs1+$imm, $rs2">;
-  def STW  : S32Inst<(outs), (ins GPR:$rs1, GPR:$rs2, i32imm:$imm), "stw $rs1+$imm, $rs2">;
+  def STB  : S32Inst<(outs), (ins GPR:$rs1, GPR:$rs2, simm12:$imm), "stb $rs1+$imm, $rs2">;
+  def STH  : S32Inst<(outs), (ins GPR:$rs1, GPR:$rs2, simm12:$imm), "sth $rs1+$imm, $rs2">;
+  def STW  : S32Inst<(outs), (ins GPR:$rs1, GPR:$rs2, simm12:$imm), "stw $rs1+$imm, $rs2">;
 }
 
 // Branch instructions
@@ -147,43 +147,16 @@ def : Pat<(setule GPR:$a, GPR:$b), (SLEU GPR:$a, GPR:$b)>;
 
 // Patterns for loading constants
 // Small constants that fit in 16 bits (signed or unsigned)
-def : Pat<(i32 simm16:$val), (ADDI R0, simm16:$val)>;
-
-// For any constant (fallback pattern using ORI for now)
-// This will truncate large constants but at least won't crash
-def : Pat<(i32 imm:$val), (ORI R0, imm:$val)>;
+def : Pat<(i32 simm12:$val), (ADDI R0, simm12:$val)>;
 
 // Patterns for %hi/%lo addressing with global addresses
 def : Pat<(SLOW32hi tglobaladdr:$addr), (LUI tglobaladdr:$addr)>;
-def : Pat<(SLOW32lo tglobaladdr:$addr), (ORI R0, tglobaladdr:$addr)>;
-
-// Patterns for %hi/%lo addressing with external symbols
 def : Pat<(SLOW32hi texternalsym:$addr), (LUI texternalsym:$addr)>;
-def : Pat<(SLOW32lo texternalsym:$addr), (ORI R0, texternalsym:$addr)>;
-
-// Patterns for %hi/%lo addressing with jump tables
 def : Pat<(SLOW32hi tjumptable:$addr), (LUI tjumptable:$addr)>;
-def : Pat<(SLOW32lo tjumptable:$addr), (ORI R0, tjumptable:$addr)>;
-
-// Pattern for combining %hi and %lo into a full address using OR
-// Note: The LOAD_ADDR pseudo instruction handles global addresses
-// to ensure correct instruction ordering (LUI before ORI).
-def : Pat<(or (SLOW32hi tglobaladdr:$addr), (SLOW32lo tglobaladdr:$addr)),
-          (LOAD_ADDR tglobaladdr:$addr)>;
-def : Pat<(or (SLOW32lo tglobaladdr:$addr), (SLOW32hi tglobaladdr:$addr)),
-          (LOAD_ADDR tglobaladdr:$addr)>;
-
-// For external symbols, we still use direct patterns.
-def : Pat<(or (SLOW32hi texternalsym:$addr), (SLOW32lo texternalsym:$addr)),
-          (ORI (LUI texternalsym:$addr), texternalsym:$addr)>;
-def : Pat<(or (SLOW32lo texternalsym:$addr), (SLOW32hi texternalsym:$addr)),
-          (ORI (LUI texternalsym:$addr), texternalsym:$addr)>;
-
-// For jump tables
-def : Pat<(or (SLOW32hi tjumptable:$addr), (SLOW32lo tjumptable:$addr)),
-          (ORI (LUI tjumptable:$addr), tjumptable:$addr)>;
-def : Pat<(or (SLOW32lo tjumptable:$addr), (SLOW32hi tjumptable:$addr)),
-          (ORI (LUI tjumptable:$addr), tjumptable:$addr)>;
+
+// DAG lowering now materialises globals via the LOAD_ADDR pseudo, so no extra
+// combination patterns are required here.
+
 
 // Patterns for loads and stores
 def : Pat<(load GPR:$addr), (LDW GPR:$addr, 0)>;
@@ -191,28 +164,28 @@ def : Pat<(store GPR:$val, GPR:$addr), (STW GPR:$addr, GPR:$val, 0)>;
 
 // Patterns for loads with offsets (base + immediate)
 // Also use ComplexPattern for better address matching
-def : Pat<(load (add GPR:$base, simm16:$offset)), 
-          (LDW GPR:$base, simm16:$offset)>;
-def : Pat<(sextloadi8 (add GPR:$base, simm16:$offset)), 
-          (LDB GPR:$base, simm16:$offset)>;
-def : Pat<(zextloadi8 (add GPR:$base, simm16:$offset)), 
-          (LDBU GPR:$base, simm16:$offset)>;
-def : Pat<(extloadi8 (add GPR:$base, simm16:$offset)), 
-          (LDBU GPR:$base, simm16:$offset)>;
-def : Pat<(sextloadi16 (add GPR:$base, simm16:$offset)), 
-          (LDH GPR:$base, simm16:$offset)>;
-def : Pat<(zextloadi16 (add GPR:$base, simm16:$offset)), 
-          (LDHU GPR:$base, simm16:$offset)>;
-def : Pat<(extloadi16 (add GPR:$base, simm16:$offset)), 
-          (LDHU GPR:$base, simm16:$offset)>;
+def : Pat<(load (add GPR:$base, simm12:$offset)), 
+          (LDW GPR:$base, simm12:$offset)>;
+def : Pat<(sextloadi8 (add GPR:$base, simm12:$offset)), 
+          (LDB GPR:$base, simm12:$offset)>;
+def : Pat<(zextloadi8 (add GPR:$base, simm12:$offset)), 
+          (LDBU GPR:$base, simm12:$offset)>;
+def : Pat<(extloadi8 (add GPR:$base, simm12:$offset)), 
+          (LDBU GPR:$base, simm12:$offset)>;
+def : Pat<(sextloadi16 (add GPR:$base, simm12:$offset)), 
+          (LDH GPR:$base, simm12:$offset)>;
+def : Pat<(zextloadi16 (add GPR:$base, simm12:$offset)), 
+          (LDHU GPR:$base, simm12:$offset)>;
+def : Pat<(extloadi16 (add GPR:$base, simm12:$offset)), 
+          (LDHU GPR:$base, simm12:$offset)>;
 
 // Patterns for stores with offsets (base + immediate)
-def : Pat<(store GPR:$val, (add GPR:$base, simm16:$offset)), 
-          (STW GPR:$base, GPR:$val, simm16:$offset)>;
-def : Pat<(truncstorei8 GPR:$val, (add GPR:$base, simm16:$offset)), 
-          (STB GPR:$base, GPR:$val, simm16:$offset)>;
-def : Pat<(truncstorei16 GPR:$val, (add GPR:$base, simm16:$offset)), 
-          (STH GPR:$base, GPR:$val, simm16:$offset)>;
+def : Pat<(store GPR:$val, (add GPR:$base, simm12:$offset)), 
+          (STW GPR:$base, GPR:$val, simm12:$offset)>;
+def : Pat<(truncstorei8 GPR:$val, (add GPR:$base, simm12:$offset)), 
+          (STB GPR:$base, GPR:$val, simm12:$offset)>;
+def : Pat<(truncstorei16 GPR:$val, (add GPR:$base, simm12:$offset)), 
+          (STH GPR:$base, GPR:$val, simm12:$offset)>;
 
 // Patterns for byte loads (sign-extended and zero-extended)
 // Keep the simple patterns with explicit addressing modes
@@ -239,8 +212,8 @@ def : Pat<(sext_inreg GPR:$src, i16),
 
 // Pattern for adding small constant to byte and sign-extending
 // This handles the common pattern: (sext_inreg (add reg, small_const), i8)
-def : Pat<(sext_inreg (add GPR:$src, simm16:$imm), i8),
-          (SRAI (SLLI (ADDI GPR:$src, simm16:$imm), 24), 24)>;
+def : Pat<(sext_inreg (add GPR:$src, simm12:$imm), i8),
+          (SRAI (SLLI (ADDI GPR:$src, simm12:$imm), 24), 24)>;
 // Removed: LLVM canonicalizes immediates to RHS
 
 // For global addresses, use %hi/%lo for proper 32-bit addressing
@@ -256,10 +229,10 @@ def : Pat<(store GPR:$val, (i32 tglobaladdr:$addr)),
           (STW (LOAD_ADDR tglobaladdr:$addr), GPR:$val, 0)>;
 
 // Patterns for immediate operations
-def : Pat<(add GPR:$a, simm16:$b), (ADDI GPR:$a, simm16:$b)>;
-def : Pat<(and GPR:$a, uimm16:$b), (ANDI GPR:$a, uimm16:$b)>;
-def : Pat<(or  GPR:$a, uimm16:$b), (ORI  GPR:$a, uimm16:$b)>;
-def : Pat<(xor GPR:$a, uimm16:$b), (XORI GPR:$a, uimm16:$b)>;
+def : Pat<(add GPR:$a, simm12:$b), (ADDI GPR:$a, simm12:$b)>;
+def : Pat<(and GPR:$a, uimm12:$b), (ANDI GPR:$a, uimm12:$b)>;
+def : Pat<(or  GPR:$a, uimm12:$b), (ORI  GPR:$a, uimm12:$b)>;
+def : Pat<(xor GPR:$a, uimm12:$b), (XORI GPR:$a, uimm12:$b)>;
 
 // For large constants that don't fit in 16 bits, materialize them in a register first
 // This will be handled by the constant materialization patterns below
@@ -274,7 +247,7 @@ def LO16 : SDNodeXForm<imm, [{
 
 // Patterns for materializing constants
 // Small constants that fit in 16 bits (signed)
-def : Pat<(i32 simm16:$imm), (ADDI R0, simm16:$imm)>;
+def : Pat<(i32 simm12:$imm), (ADDI R0, simm12:$imm)>;
 
 // Constants that need LUI only (upper 16 bits, lower 16 bits are 0)
 def HI16_ONLY : PatLeaf<(imm), [{
@@ -420,16 +393,16 @@ let isCall = 1, Defs = [R1, R2, R3, R4, R5, R6, R7, R8, R9, R10, R31], Uses = [R
 //===----------------------------------------------------------------------===//
 
 // Byte and halfword load patterns with different extensions
-def : Pat<(i32 (extloadi8 (add GPR:$rs1, simm16:$off))),
-          (LDB GPR:$rs1, simm16:$off)>;
-def : Pat<(i32 (extloadi16 (add GPR:$rs1, simm16:$off))),
-          (LDH GPR:$rs1, simm16:$off)>;
+def : Pat<(i32 (extloadi8 (add GPR:$rs1, simm12:$off))),
+          (LDB GPR:$rs1, simm12:$off)>;
+def : Pat<(i32 (extloadi16 (add GPR:$rs1, simm12:$off))),
+          (LDH GPR:$rs1, simm12:$off)>;
 
 // Zero extension patterns
-def : Pat<(i32 (zextloadi8 (add GPR:$rs1, simm16:$off))),
-          (LDBU GPR:$rs1, simm16:$off)>;
-def : Pat<(i32 (zextloadi16 (add GPR:$rs1, simm16:$off))),
-          (LDHU GPR:$rs1, simm16:$off)>;
+def : Pat<(i32 (zextloadi8 (add GPR:$rs1, simm12:$off))),
+          (LDBU GPR:$rs1, simm12:$off)>;
+def : Pat<(i32 (zextloadi16 (add GPR:$rs1, simm12:$off))),
+          (LDHU GPR:$rs1, simm12:$off)>;
 
 // Sign extension from i8/i16 without shift sequences
 def : Pat<(i32 (sext_inreg GPR:$rs1, i8)),
@@ -438,8 +411,8 @@ def : Pat<(i32 (sext_inreg GPR:$rs1, i16)),
           (SRA (SLL GPR:$rs1, 16), 16)>;
 
 // Patterns for adding immediates to different types
-def : Pat<(i32 (add GPR:$rs1, simm16:$imm)),
-          (ADDI GPR:$rs1, simm16:$imm)>;
+def : Pat<(i32 (add GPR:$rs1, simm12:$imm)),
+          (ADDI GPR:$rs1, simm12:$imm)>;
 
 
 // Load/store pairs for memcpy optimization
@@ -447,19 +420,19 @@ def : Pat<(i32 (add GPR:$rs1, simm16:$imm)),
 multiclass LoadStorePat<ValueType vt, PatFrag ldop, PatFrag stop, 
                         S32Inst ldinst, S32Inst stinst> {
   def : Pat<(vt (ldop GPR:$base)), (ldinst GPR:$base, 0)>;
-  def : Pat<(vt (ldop (add GPR:$base, simm16:$off))), 
-            (ldinst GPR:$base, simm16:$off)>;
+  def : Pat<(vt (ldop (add GPR:$base, simm12:$off))), 
+            (ldinst GPR:$base, simm12:$off)>;
   def : Pat<(stop vt:$val, GPR:$base), (stinst GPR:$base, 0, vt:$val)>;
-  def : Pat<(stop vt:$val, (add GPR:$base, simm16:$off)), 
-            (stinst GPR:$base, simm16:$off, vt:$val)>;
+  def : Pat<(stop vt:$val, (add GPR:$base, simm12:$off)), 
+            (stinst GPR:$base, simm12:$off, vt:$val)>;
 }
 
 defm : LoadStorePat<i32, load, store, LDW, STW>;
 
 // Patterns for constant materialization
 def : Pat<(i32 0), (ADDI R0, 0)>;
-def : Pat<(i32 simm16:$imm), (ADDI R0, simm16:$imm)>;
-def : Pat<(i32 uimm16:$imm), (ORI R0, uimm16:$imm)>;
+def : Pat<(i32 simm12:$imm), (ADDI R0, simm12:$imm)>;
+def : Pat<(i32 uimm12:$imm), (ORI R0, uimm12:$imm)>;
 
 // Upper 16-bit constant patterns
 def : Pat<(i32 (shl (i32 imm:$imm), (i32 16))),
@@ -502,4 +475,3 @@ def : Pat<(subc GPR:$lhs, GPR:$rhs),
           (SUBC_PSEUDO GPR:$lhs, GPR:$rhs)>;
 def : Pat<(sube GPR:$lhs, GPR:$rhs),
           (SUBE_PSEUDO GPR:$lhs, GPR:$rhs, R0)>;
-
diff --git a/llvm/lib/Target/SLOW32/SLOW32LoadAddrOpt.cpp b/llvm/lib/Target/SLOW32/SLOW32LoadAddrOpt.cpp
new file mode 100644
index 000000000..2016f99f8
--- /dev/null
+++ b/llvm/lib/Target/SLOW32/SLOW32LoadAddrOpt.cpp
@@ -0,0 +1,153 @@
+//===- SLOW32LoadAddrOpt.cpp - Fold LOAD_ADDR addends --------------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// A simple MachineFunction pass that canonicalises address materialisation by
+// folding `ADDI` immediates into the preceding `LOAD_ADDR` pseudo. This keeps
+// global+constant forms flowing through to the post-RA expansion so we always
+// emit a single LUI/ADDI pair with the final relocation.
+//
+//===----------------------------------------------------------------------===//
+
+#include "SLOW32.h"
+#include "SLOW32InstrInfo.h"
+#include "SLOW32Subtarget.h"
+#include "llvm/CodeGen/MachineFunctionPass.h"
+#include "llvm/CodeGen/MachineInstrBuilder.h"
+#include "llvm/CodeGen/MachineRegisterInfo.h"
+#include "llvm/CodeGen/TargetInstrInfo.h"
+#include "llvm/InitializePasses.h"
+#include "llvm/Support/Debug.h"
+
+using namespace llvm;
+
+#define DEBUG_TYPE "slow32-load-addr-opt"
+
+namespace {
+
+class SLOW32LoadAddrOpt : public MachineFunctionPass {
+public:
+  static char ID;
+  SLOW32LoadAddrOpt() : MachineFunctionPass(ID) {
+    initializeSLOW32LoadAddrOptPass(*PassRegistry::getPassRegistry());
+  }
+
+  StringRef getPassName() const override {
+    return "SLOW32 load address folding";
+  }
+
+  bool runOnMachineFunction(MachineFunction &MF) override;
+
+private:
+  bool foldAddIntoLoadAddr(MachineInstr &Add, MachineRegisterInfo &MRI,
+                           const TargetInstrInfo *TII) const;
+};
+
+} // namespace
+
+char SLOW32LoadAddrOpt::ID = 0;
+
+INITIALIZE_PASS(SLOW32LoadAddrOpt, DEBUG_TYPE, "SLOW32 load address folding",
+                false, false)
+
+static bool canAdjustOperand(const MachineOperand &Op) {
+  return Op.isGlobal() || Op.isSymbol() || Op.isMCSymbol() ||
+         Op.isBlockAddress() || Op.isCPI();
+}
+
+static bool adjustOperandOffset(MachineOperand &Op, int64_t Delta) {
+  if (Delta == 0)
+    return true;
+
+  if (Op.isGlobal() || Op.isSymbol() || Op.isMCSymbol() || Op.isBlockAddress() ||
+      Op.isCPI()) {
+    Op.setOffset(Op.getOffset() + Delta);
+    return true;
+  }
+
+  return false;
+}
+
+bool SLOW32LoadAddrOpt::foldAddIntoLoadAddr(MachineInstr &Add,
+                                            MachineRegisterInfo &MRI,
+                                            const TargetInstrInfo *TII) const {
+  if (Add.getOpcode() != SLOW32::ADDI)
+    return false;
+
+  if (!Add.getOperand(1).isReg() || !Add.getOperand(2).isImm())
+    return false;
+
+  Register BaseReg = Add.getOperand(1).getReg();
+  MachineInstr *Def = nullptr;
+
+  if (BaseReg.isVirtual())
+    Def = MRI.getVRegDef(BaseReg);
+  else if (Register::isPhysicalRegister(BaseReg))
+    return false;
+
+  if (!Def || Def->getOpcode() != SLOW32::LOAD_ADDR)
+    return false;
+
+  MachineOperand &SymOp = Def->getOperand(1);
+  if (!canAdjustOperand(SymOp))
+    return false;
+
+  int64_t Imm = Add.getOperand(2).getImm();
+
+  Register DstReg = Add.getOperand(0).getReg();
+
+  // Case 1: the ADDI writes back into the same register and the original
+  // LOAD_ADDR has no other users. In that case we can mutate the LOAD_ADDR in
+  // place and erase the ADDI entirely.
+  if (DstReg == BaseReg && MRI.hasOneNonDBGUse(BaseReg)) {
+    if (adjustOperandOffset(SymOp, Imm)) {
+      LLVM_DEBUG(dbgs() << "SLOW32LoadAddrOpt: folded ADDI into defining "
+                        "LOAD_ADDR (in-place)\n");
+      Add.eraseFromParent();
+      return true;
+    }
+    return false;
+  }
+
+  // Otherwise synthesise a fresh LOAD_ADDR carrying the combined offset so the
+  // ADDI can be removed without perturbing other uses of the original result.
+  MachineOperand NewOp = SymOp;
+  if (!adjustOperandOffset(NewOp, Imm))
+    return false;
+
+  MachineBasicBlock &MBB = *Add.getParent();
+  MachineInstrBuilder MIB =
+      BuildMI(MBB, Add, Add.getDebugLoc(), TII->get(SLOW32::LOAD_ADDR), DstReg);
+  MIB.add(NewOp);
+
+  LLVM_DEBUG(dbgs() << "SLOW32LoadAddrOpt: materialised LOAD_ADDR with folded "
+                    "offset for ADDI\n");
+
+  Add.eraseFromParent();
+  return true;
+}
+
+bool SLOW32LoadAddrOpt::runOnMachineFunction(MachineFunction &MF) {
+  const SLOW32Subtarget &STI = MF.getSubtarget<SLOW32Subtarget>();
+  const TargetInstrInfo *TII = STI.getInstrInfo();
+  MachineRegisterInfo &MRI = MF.getRegInfo();
+
+  bool Changed = false;
+  for (MachineBasicBlock &MBB : MF) {
+    for (MachineBasicBlock::iterator MII = MBB.begin(), E = MBB.end();
+         MII != E;) {
+      MachineInstr &MI = *MII++;
+      Changed |= foldAddIntoLoadAddr(MI, MRI, TII);
+    }
+  }
+
+  return Changed;
+}
+
+FunctionPass *llvm::createSLOW32LoadAddrOptPass() {
+  return new SLOW32LoadAddrOpt();
+}
diff --git a/llvm/lib/Target/SLOW32/SLOW32RegisterInfo.cpp b/llvm/lib/Target/SLOW32/SLOW32RegisterInfo.cpp
index 33d90d7fd..6af375bb6 100644
--- a/llvm/lib/Target/SLOW32/SLOW32RegisterInfo.cpp
+++ b/llvm/lib/Target/SLOW32/SLOW32RegisterInfo.cpp
@@ -20,6 +20,7 @@
 #include "llvm/CodeGen/TargetFrameLowering.h"
 #include "llvm/CodeGen/TargetInstrInfo.h"
 #include "llvm/Support/ErrorHandling.h"
+#include <algorithm>
 
 #define GET_REGINFO_TARGET_DESC
 #include "SLOW32GenRegisterInfo.inc"
@@ -57,6 +58,8 @@ bool SLOW32RegisterInfo::eliminateFrameIndex(MachineBasicBlock::iterator II,
   MachineBasicBlock &MBB = *MI.getParent();
   MachineFunction &MF = *MBB.getParent();
   const MachineFrameInfo &MFI = MF.getFrameInfo();
+  const TargetInstrInfo *TII = MF.getSubtarget().getInstrInfo();
+  MachineRegisterInfo &MRI = MF.getRegInfo();
   
   // Get the frame index operand
   MachineOperand &FIOp = MI.getOperand(FIOperandNum);
@@ -65,20 +68,78 @@ bool SLOW32RegisterInfo::eliminateFrameIndex(MachineBasicBlock::iterator II,
   // Calculate the actual offset from the frame pointer
   int64_t Offset = MFI.getObjectOffset(FrameIndex);
   
-  // For ADDI, the frame index should be the second operand (the immediate)
-  // The first operand should already be the frame pointer (R30)
-  if (MI.getOpcode() == SLOW32::ADDI) {
-    // Just change the frame index to the calculated offset
-    FIOp.ChangeToImmediate(Offset);
-  } else {
-    // For other instructions (loads/stores), we may need different handling
-    // For now, just change to immediate
-    FIOp.ChangeToImmediate(Offset);
+  // Stack pointer adjustments between call frames are already baked into the
+  // prologue/epilogue. Just account for them here so we keep consistent offsets.
+  Offset += SPAdj;
+
+  // Frame indexes currently enter without an additional offset. If we start
+  // using addFrameIndex with non-zero displacements we will thread that here.
+
+  // Ensure the base+offset fits in the 12-bit signed window supported by the
+  // instruction. If not, materialise the high part using additional ADDI nodes
+  // so the final memory op keeps a legal displacement.
+  MachineOperand &BaseOp = MI.getOperand(FIOperandNum - 1);
+  Register BaseReg = BaseOp.getReg();
+  DebugLoc DL = MI.getDebugLoc();
+
+  if (!isInt<12>(Offset)) {
+    if (RS) {
+      if (!BaseReg.isPhysical())
+        report_fatal_error("Frame index elimination requires physical base regs");
+
+      unsigned Scav = RS->scavengeRegisterBackwards(SLOW32::GPRRegClass, II,
+                                                    false, SPAdj);
+      if (!Scav)
+        report_fatal_error("Unable to scavenge register for frame index");
+
+      Register Scratch = Register(Scav);
+      BuildMI(MBB, II, DL, TII->get(SLOW32::ADD), Scratch)
+          .addReg(BaseReg)
+          .addReg(SLOW32::R0);
+
+      int64_t Remaining = Offset;
+      while (!isInt<12>(Remaining)) {
+        int64_t Step = Remaining > 0 ? std::min<int64_t>(Remaining, 2047)
+                                     : std::max<int64_t>(Remaining, -2048);
+        BuildMI(MBB, II, DL, TII->get(SLOW32::ADDI), Scratch)
+            .addReg(Scratch)
+            .addImm(Step);
+        Remaining -= Step;
+      }
+
+      Offset = Remaining;
+      BaseOp.setReg(Scratch);
+      BaseOp.setIsKill(false);
+      RS->setRegUsed(Scav);
+    } else {
+      Register CurrBase = BaseReg;
+      int64_t Remaining = Offset;
+      while (!isInt<12>(Remaining)) {
+        int64_t Step = Remaining > 0 ? std::min<int64_t>(Remaining, 2047)
+                                     : std::max<int64_t>(Remaining, -2048);
+        Register NextBase = MRI.createVirtualRegister(&SLOW32::GPRRegClass);
+        BuildMI(MBB, II, DL, TII->get(SLOW32::ADDI), NextBase)
+            .addReg(CurrBase)
+            .addImm(Step);
+        CurrBase = NextBase;
+        Remaining -= Step;
+      }
+
+      Offset = Remaining;
+      BaseOp.setReg(CurrBase);
+      BaseOp.setIsKill(false);
+    }
   }
+
+  FIOp.ChangeToImmediate(Offset);
   
   return false;
 }
 
 Register SLOW32RegisterInfo::getFrameRegister(const MachineFunction &MF) const {
   return SLOW32::R30;  // FP register
-}
\ No newline at end of file
+}
+
+bool SLOW32RegisterInfo::requiresRegisterScavenging(const MachineFunction &MF) const {
+  return true;
+}
diff --git a/llvm/lib/Target/SLOW32/SLOW32RegisterInfo.h b/llvm/lib/Target/SLOW32/SLOW32RegisterInfo.h
index 93aae76e3..4a3e7e7a7 100644
--- a/llvm/lib/Target/SLOW32/SLOW32RegisterInfo.h
+++ b/llvm/lib/Target/SLOW32/SLOW32RegisterInfo.h
@@ -29,8 +29,10 @@ public:
                           RegScavenger *RS = nullptr) const override;
   
   Register getFrameRegister(const MachineFunction &MF) const override;
+
+  bool requiresRegisterScavenging(const MachineFunction &MF) const override;
 };
 
 } // end namespace llvm
 
-#endif
\ No newline at end of file
+#endif
diff --git a/llvm/lib/Target/SLOW32/SLOW32RegisterInfo.td b/llvm/lib/Target/SLOW32/SLOW32RegisterInfo.td
index 63b1c1b80..1361b9d9f 100644
--- a/llvm/lib/Target/SLOW32/SLOW32RegisterInfo.td
+++ b/llvm/lib/Target/SLOW32/SLOW32RegisterInfo.td
@@ -1,61 +1,50 @@
 // SLOW32 Register definitions
 // No include needed here - Target.td is already included in SLOW32.td
 
-class GPR<string n, bits<5> num> : Register<n> { 
+class GPR<string n, bits<5> num, list<string> alt = []> : Register<n> { 
   let Namespace = "SLOW32";
+  let AltNames = alt;
   let HWEncoding{4-0} = num;
   let HWEncoding{15-5} = 0;
 }
 
-// Define all 32 registers
-def R0  : GPR<"r0",  0>;  def R1  : GPR<"r1",  1>;  def R2  : GPR<"r2",  2>;
-def R3  : GPR<"r3",  3>;  def R4  : GPR<"r4",  4>;  def R5  : GPR<"r5",  5>;
-def R6  : GPR<"r6",  6>;  def R7  : GPR<"r7",  7>;  def R8  : GPR<"r8",  8>;
-def R9  : GPR<"r9",  9>;  def R10 : GPR<"r10", 10>; def R11 : GPR<"r11", 11>;
-def R12 : GPR<"r12", 12>; def R13 : GPR<"r13", 13>; def R14 : GPR<"r14", 14>;
-def R15 : GPR<"r15", 15>; def R16 : GPR<"r16", 16>; def R17 : GPR<"r17", 17>;
-def R18 : GPR<"r18", 18>; def R19 : GPR<"r19", 19>; def R20 : GPR<"r20", 20>;
-def R21 : GPR<"r21", 21>; def R22 : GPR<"r22", 22>; def R23 : GPR<"r23", 23>;
-def R24 : GPR<"r24", 24>; def R25 : GPR<"r25", 25>; def R26 : GPR<"r26", 26>;
-def R27 : GPR<"r27", 27>; def R28 : GPR<"r28", 28>; def R29 : GPR<"sp", 29>;
-def R30 : GPR<"fp", 30>;  def R31 : GPR<"lr", 31>;
-
-// Register aliases matching SLOW-32 conventions
-def ZERO : GPR<"zero", 0>;
-def RV   : GPR<"rv", 1>;
-def T0   : GPR<"t0", 2>;
-def A0   : GPR<"a0", 3>;
-def A1   : GPR<"a1", 4>;
-def A2   : GPR<"a2", 5>;
-def A3   : GPR<"a3", 6>;
-def A4   : GPR<"a4", 7>;
-def A5   : GPR<"a5", 8>;
-def A6   : GPR<"a6", 9>;
-def A7   : GPR<"a7", 10>;
-def S0   : GPR<"s0", 11>;
-def S1   : GPR<"s1", 12>;
-def S2   : GPR<"s2", 13>;
-def S3   : GPR<"s3", 14>;
-def S4   : GPR<"s4", 15>;
-def S5   : GPR<"s5", 16>;
-def S6   : GPR<"s6", 17>;
-def S7   : GPR<"s7", 18>;
-def S8   : GPR<"s8", 19>;
-def S9   : GPR<"s9", 20>;
-def S10  : GPR<"s10", 21>;
-def S11  : GPR<"s11", 22>;
-def S12  : GPR<"s12", 23>;
-def S13  : GPR<"s13", 24>;
-def S14  : GPR<"s14", 25>;
-def S15  : GPR<"s15", 26>;
-def S16  : GPR<"s16", 27>;
-def S17  : GPR<"s17", 28>;
-def SP   : GPR<"sp", 29>;
-def FP   : GPR<"fp", 30>;
-def LR   : GPR<"lr", 31>;
+// Define all 32 registers (ABI aliases in AltNames)
+def R0  : GPR<"r0",  0, ["zero"]>;
+def R1  : GPR<"r1",  1, ["rv"]>;
+def R2  : GPR<"r2",  2, ["t0"]>;
+def R3  : GPR<"r3",  3, ["a0"]>;
+def R4  : GPR<"r4",  4, ["a1"]>;
+def R5  : GPR<"r5",  5, ["a2"]>;
+def R6  : GPR<"r6",  6, ["a3"]>;
+def R7  : GPR<"r7",  7, ["a4"]>;
+def R8  : GPR<"r8",  8, ["a5"]>;
+def R9  : GPR<"r9",  9, ["a6"]>;
+def R10 : GPR<"r10", 10, ["a7"]>;
+def R11 : GPR<"r11", 11, ["s0"]>;
+def R12 : GPR<"r12", 12, ["s1"]>;
+def R13 : GPR<"r13", 13, ["s2"]>;
+def R14 : GPR<"r14", 14, ["s3"]>;
+def R15 : GPR<"r15", 15, ["s4"]>;
+def R16 : GPR<"r16", 16, ["s5"]>;
+def R17 : GPR<"r17", 17, ["s6"]>;
+def R18 : GPR<"r18", 18, ["s7"]>;
+def R19 : GPR<"r19", 19, ["s8"]>;
+def R20 : GPR<"r20", 20, ["s9"]>;
+def R21 : GPR<"r21", 21, ["s10"]>;
+def R22 : GPR<"r22", 22, ["s11"]>;
+def R23 : GPR<"r23", 23, ["s12"]>;
+def R24 : GPR<"r24", 24, ["s13"]>;
+def R25 : GPR<"r25", 25, ["s14"]>;
+def R26 : GPR<"r26", 26, ["s15"]>;
+def R27 : GPR<"r27", 27, ["s16"]>;
+def R28 : GPR<"r28", 28, ["s17"]>;
+def R29 : GPR<"sp", 29>;
+def R30 : GPR<"fp", 30>;
+def R31 : GPR<"lr", 31>;
 
 // Define register class
 def GPR : RegisterClass<"SLOW32", [i32], 32, (add 
   R0, R1, R2, R3, R4, R5, R6, R7, R8, R9, R10,
   R11, R12, R13, R14, R15, R16, R17, R18, R19, R20, 
-  R21, R22, R23, R24, R25, R26, R27, R28, R29, R30, R31)>;
\ No newline at end of file
+  R21, R22, R23, R24, R25, R26, R27, R28, R29, R30, R31)>;
+
diff --git a/llvm/lib/Target/SLOW32/SLOW32TargetMachine.cpp b/llvm/lib/Target/SLOW32/SLOW32TargetMachine.cpp
index 9d79aebf4..48f5811a2 100644
--- a/llvm/lib/Target/SLOW32/SLOW32TargetMachine.cpp
+++ b/llvm/lib/Target/SLOW32/SLOW32TargetMachine.cpp
@@ -14,12 +14,14 @@
 #include "llvm/CodeGen/Passes.h"
 #include "llvm/CodeGen/TargetLoweringObjectFileImpl.h"
 #include "llvm/CodeGen/TargetPassConfig.h"
+#include "llvm/InitializePasses.h"
 #include "llvm/MC/TargetRegistry.h"
 
 using namespace llvm;
 
 extern "C" LLVM_EXTERNAL_VISIBILITY void LLVMInitializeSLOW32Target() {
   RegisterTargetMachine<SLOW32TargetMachine> X(getTheSLOW32Target());
+  initializeSLOW32LoadAddrOptPass(*PassRegistry::getPassRegistry());
 }
 
 static std::string computeDataLayout() {
@@ -72,6 +74,7 @@ bool SLOW32PassConfig::addInstSelector() {
 }
 
 void SLOW32PassConfig::addPreRegAlloc() {
+  addPass(createSLOW32LoadAddrOptPass());
   TargetPassConfig::addPreRegAlloc();
 }
 
@@ -80,4 +83,4 @@ MachineFunctionInfo *SLOW32TargetMachine::createMachineFunctionInfo(
     const TargetSubtargetInfo *STI) const {
   return SLOW32MachineFunctionInfo::create<SLOW32MachineFunctionInfo>(
       Allocator, F, STI);
-}
\ No newline at end of file
+}
diff --git a/llvm/lib/Target/SLOW32/STATUS.md b/llvm/lib/Target/SLOW32/STATUS.md
index 242324037..c852fe1af 100644
--- a/llvm/lib/Target/SLOW32/STATUS.md
+++ b/llvm/lib/Target/SLOW32/STATUS.md
@@ -41,7 +41,7 @@ The SLOW32 LLVM backend achieved remarkable completeness before being mothballed
   - Halfword: LDH (sign-extend), LDHU (zero-extend), STH
 - **Comparison**: SLT, SLTU, SEQ, SNE, SGT, SGTU, SGE, SGEU
 - **Control Flow**: JAL, JALR for calls; BEQ, BNE, BLT, BGE for branches
-- **Constants**: LI (pseudo), LUI, ORI for loading immediates
+- **Constants**: LI (pseudo), LUI, ADDI for loading immediates
 
 ### Function Support
 - Function prologue/epilogue generation
@@ -63,7 +63,7 @@ The SLOW32 LLVM backend achieved remarkable completeness before being mothballed
 ## ⚠️ Partially Working
 
 ### %hi/%lo Addressing
-- Generates correct LUI + ORI sequences
+- Generates correct LUI + ADDI sequences with 12-bit signed immediates
 - Relocations work at binary level
 - ✅ Assembly syntax now properly emits %hi() and %lo() markers
 - Distinguishes between function addresses (direct) and data addresses (%hi/%lo)
@@ -257,7 +257,7 @@ All 8 regression tests with test files passing:
 - ✅ **Switch statements** with jump tables
 - ✅ **All optimization levels** working at stable commit (417a5a678)
 - ✅ **Function pointers** and indirect calls
-- ✅ **Inline assembly** with basic constraints
+- ✅ **Inline assembly** with register/immediate/memory constraints
 - ✅ **14/14 regression tests passing** at stable commit
 
 ### Known Issues at Latest LLVM Main
@@ -274,4 +274,4 @@ This project was a remarkable collaboration between human expertise and AI assis
 
 ---
 *Backend mothballed 2025-09-15. Use commit 417a5a678 for stable version.*
-*Waiting for LLVM 22 stabilization before continuing development.*
\ No newline at end of file
+*Waiting for LLVM 22 stabilization before continuing development.*
